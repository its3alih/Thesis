{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoGXPT6WpaeFW89STqAmV4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its3alih/Thesis/blob/main/BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FIRST"
      ],
      "metadata": {
        "id": "5EvIL7LqkOxh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3i9_ZZljtxg",
        "outputId": "76978a17-c584-46d6-da40-7945708ad46d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1485\n",
            "Epoch 2/10, Loss: 0.0362\n",
            "Epoch 3/10, Loss: 0.0186\n",
            "Epoch 4/10, Loss: 0.0107\n",
            "Epoch 5/10, Loss: 0.0059\n",
            "Epoch 6/10, Loss: 0.0034\n",
            "Epoch 7/10, Loss: 0.0018\n",
            "Epoch 8/10, Loss: 0.0009\n",
            "Epoch 9/10, Loss: 0.0007\n",
            "Epoch 10/10, Loss: 0.0004\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I     0.9898    0.9327    0.9604       520\n",
            "           O     0.9968    0.9995    0.9982     11010\n",
            "\n",
            "    accuracy                         0.9965     11530\n",
            "   macro avg     0.9933    0.9661    0.9793     11530\n",
            "weighted avg     0.9965    0.9965    0.9965     11530\n",
            "\n",
            "Accuracy: 0.9965\n",
            "Precision: 0.9965\n",
            "Recall: 0.9965\n",
            "F1 Score: 0.9965\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load Data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # Use -100 to ignore in loss\n",
        "\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # shape: (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with detailed metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # Ignore padding tokens\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IO.xlsx\"  # Your IO tagging dataset path\n",
        "    sentences, tags = load_excel_data(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IE Data (adapted for IE tagging)\n",
        "def load_excel_data_ie(file_path):\n",
        "    # Assuming same format as IO but with IE tags instead\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # Use -100 to ignore in loss\n",
        "\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # shape: (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with detailed metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # Ignore padding tokens\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IE.xlsx\"  # Your IE tagging dataset path\n",
        "    sentences, tags = load_excel_data_ie(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3he4avVnKPv",
        "outputId": "f643ca66-41fb-4bee-ba0e-f33211533710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3592\n",
            "Epoch 2/10, Loss: 0.0870\n",
            "Epoch 3/10, Loss: 0.0413\n",
            "Epoch 4/10, Loss: 0.0236\n",
            "Epoch 5/10, Loss: 0.0141\n",
            "Epoch 6/10, Loss: 0.0092\n",
            "Epoch 7/10, Loss: 0.0058\n",
            "Epoch 8/10, Loss: 0.0039\n",
            "Epoch 9/10, Loss: 0.0026\n",
            "Epoch 10/10, Loss: 0.0024\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E     0.9382    0.9247    0.9314       279\n",
            "          EO     0.9662    0.9554    0.9607       269\n",
            "           I     0.9573    0.9208    0.9387       341\n",
            "          IO     0.9955    0.9973    0.9964     11068\n",
            "\n",
            "    accuracy                         0.9925     11957\n",
            "   macro avg     0.9643    0.9496    0.9568     11957\n",
            "weighted avg     0.9924    0.9925    0.9924     11957\n",
            "\n",
            "Accuracy: 0.9925\n",
            "Precision: 0.9924\n",
            "Recall: 0.9925\n",
            "F1 Score: 0.9924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IOB Data\n",
        "def load_excel_data_iob(file_path):\n",
        "    # Assumes dataset with columns 'Word i' and 'Word i entity tag' using IOB tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IOB.xlsx\"  # Your IOB tagging dataset path\n",
        "    sentences, tags = load_excel_data_iob(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNp7UbgSn2js",
        "outputId": "cad252ac-48d9-490d-acd2-e8f382e994b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.2174\n",
            "Epoch 2/10, Loss: 0.0461\n",
            "Epoch 3/10, Loss: 0.0222\n",
            "Epoch 4/10, Loss: 0.0121\n",
            "Epoch 5/10, Loss: 0.0073\n",
            "Epoch 6/10, Loss: 0.0043\n",
            "Epoch 7/10, Loss: 0.0025\n",
            "Epoch 8/10, Loss: 0.0014\n",
            "Epoch 9/10, Loss: 0.0010\n",
            "Epoch 10/10, Loss: 0.0008\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B     0.9922    0.9200    0.9547       275\n",
            "           I     0.9727    0.8934    0.9314       319\n",
            "           O     0.9952    0.9992    0.9972     11299\n",
            "\n",
            "    accuracy                         0.9945     11893\n",
            "   macro avg     0.9867    0.9375    0.9611     11893\n",
            "weighted avg     0.9945    0.9945    0.9944     11893\n",
            "\n",
            "Accuracy: 0.9945\n",
            "Precision: 0.9945\n",
            "Recall: 0.9945\n",
            "F1 Score: 0.9944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IOBES Data\n",
        "def load_excel_data_iobes(file_path):\n",
        "    # Assumes dataset with columns 'Word i' and 'Word i entity tag' using IOBES tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IOBES.xlsx\"  # Your IOBES tagging dataset path\n",
        "    sentences, tags = load_excel_data_iobes(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON75osk_ofom",
        "outputId": "48f931fd-bed7-4308-92ec-e746ff072ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3229\n",
            "Epoch 2/10, Loss: 0.0640\n",
            "Epoch 3/10, Loss: 0.0310\n",
            "Epoch 4/10, Loss: 0.0180\n",
            "Epoch 5/10, Loss: 0.0114\n",
            "Epoch 6/10, Loss: 0.0079\n",
            "Epoch 7/10, Loss: 0.0053\n",
            "Epoch 8/10, Loss: 0.0040\n",
            "Epoch 9/10, Loss: 0.0030\n",
            "Epoch 10/10, Loss: 0.0020\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B     0.9914    0.9274    0.9583       248\n",
            "           E     0.9741    0.9113    0.9417       248\n",
            "           I     0.9714    0.7234    0.8293        47\n",
            "           O     0.9957    0.9996    0.9976     11488\n",
            "           S     0.7500    0.6000    0.6667         5\n",
            "\n",
            "    accuracy                         0.9950     12036\n",
            "   macro avg     0.9365    0.8323    0.8787     12036\n",
            "weighted avg     0.9949    0.9950    0.9949     12036\n",
            "\n",
            "Accuracy: 0.9950\n",
            "Precision: 0.9949\n",
            "Recall: 0.9950\n",
            "F1 Score: 0.9949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IOE Data\n",
        "def load_excel_data_ioe(file_path):\n",
        "    # Assumes dataset with columns 'Word i' and 'Word i entity tag' using IOE tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IOE.xlsx\"  # Your IOE tagging dataset path\n",
        "    sentences, tags = load_excel_data_ioe(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-en1pSojpUl2",
        "outputId": "8d186e9f-01b2-482a-94db-d0d4394dd736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.2369\n",
            "Epoch 2/10, Loss: 0.0502\n",
            "Epoch 3/10, Loss: 0.0249\n",
            "Epoch 4/10, Loss: 0.0148\n",
            "Epoch 5/10, Loss: 0.0091\n",
            "Epoch 6/10, Loss: 0.0054\n",
            "Epoch 7/10, Loss: 0.0033\n",
            "Epoch 8/10, Loss: 0.0023\n",
            "Epoch 9/10, Loss: 0.0015\n",
            "Epoch 10/10, Loss: 0.0010\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E     0.9810    0.9314    0.9556       277\n",
            "           I     0.9720    0.9343    0.9528       335\n",
            "           O     0.9964    0.9989    0.9977     10903\n",
            "\n",
            "    accuracy                         0.9954     11515\n",
            "   macro avg     0.9832    0.9549    0.9687     11515\n",
            "weighted avg     0.9954    0.9954    0.9953     11515\n",
            "\n",
            "Accuracy: 0.9954\n",
            "Precision: 0.9954\n",
            "Recall: 0.9954\n",
            "F1 Score: 0.9953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load BI Data\n",
        "def load_excel_data_bi(file_path):\n",
        "    # Assumes dataset with columns 'Word i' and 'Word i entity tag' using BI tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/BI.xlsx\"  # Your BI tagging dataset path\n",
        "    sentences, tags = load_excel_data_bi(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzfPxEm2pfuE",
        "outputId": "ad379eed-4dbe-4159-e541-55f17dad7a37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3285\n",
            "Epoch 2/10, Loss: 0.0725\n",
            "Epoch 3/10, Loss: 0.0353\n",
            "Epoch 4/10, Loss: 0.0204\n",
            "Epoch 5/10, Loss: 0.0119\n",
            "Epoch 6/10, Loss: 0.0078\n",
            "Epoch 7/10, Loss: 0.0055\n",
            "Epoch 8/10, Loss: 0.0034\n",
            "Epoch 9/10, Loss: 0.0024\n",
            "Epoch 10/10, Loss: 0.0017\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B     0.9577    0.9842    0.9708       253\n",
            "          BO     0.9474    0.9340    0.9406       212\n",
            "           I     0.9585    0.9685    0.9635       286\n",
            "          IO     0.9983    0.9976    0.9979     10905\n",
            "\n",
            "    accuracy                         0.9955     11656\n",
            "   macro avg     0.9654    0.9711    0.9682     11656\n",
            "weighted avg     0.9955    0.9955    0.9955     11656\n",
            "\n",
            "Accuracy: 0.9955\n",
            "Precision: 0.9955\n",
            "Recall: 0.9955\n",
            "F1 Score: 0.9955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load BIES Data\n",
        "def load_excel_data_bies(file_path):\n",
        "    # Dataset with columns 'Word i' and 'Word i entity tag' using BIES tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/BIES.xlsx\"  # Your BIES tagging dataset path\n",
        "    sentences, tags = load_excel_data_bies(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJoVhy1vqDSm",
        "outputId": "c398ddf9-33e6-4808-8667-ebb8f5a0e456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.5207\n",
            "Epoch 2/10, Loss: 0.1246\n",
            "Epoch 3/10, Loss: 0.0596\n",
            "Epoch 4/10, Loss: 0.0381\n",
            "Epoch 5/10, Loss: 0.0254\n",
            "Epoch 6/10, Loss: 0.0174\n",
            "Epoch 7/10, Loss: 0.0118\n",
            "Epoch 8/10, Loss: 0.0096\n",
            "Epoch 9/10, Loss: 0.0067\n",
            "Epoch 10/10, Loss: 0.0050\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B     0.9640    0.9683    0.9661       221\n",
            "          BO     0.8989    0.9357    0.9169       171\n",
            "           E     0.9286    0.9412    0.9348       221\n",
            "          EO     0.9481    0.9710    0.9594       207\n",
            "           I     0.9062    0.6905    0.7838        42\n",
            "          IO     0.9967    0.9961    0.9964     10900\n",
            "           S     0.6667    0.6667    0.6667         3\n",
            "          SO     1.0000    1.0000    1.0000         4\n",
            "\n",
            "    accuracy                         0.9921     11769\n",
            "   macro avg     0.9136    0.8962    0.9030     11769\n",
            "weighted avg     0.9921    0.9921    0.9920     11769\n",
            "\n",
            "Accuracy: 0.9921\n",
            "Precision: 0.9921\n",
            "Recall: 0.9921\n",
            "F1 Score: 0.9920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SECOND"
      ],
      "metadata": {
        "id": "BWG3xYWUrAZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load Data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # Use -100 to ignore in loss\n",
        "\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # shape: (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with detailed metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # Ignore padding tokens\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IO.xlsx\"  # Your IO tagging dataset path\n",
        "    sentences, tags = load_excel_data(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of-L28tHqlSH",
        "outputId": "e0b6506a-9b6b-47f5-9e17-3beb2b63719a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1560\n",
            "Epoch 2/10, Loss: 0.0358\n",
            "Epoch 3/10, Loss: 0.0183\n",
            "Epoch 4/10, Loss: 0.0107\n",
            "Epoch 5/10, Loss: 0.0059\n",
            "Epoch 6/10, Loss: 0.0034\n",
            "Epoch 7/10, Loss: 0.0018\n",
            "Epoch 8/10, Loss: 0.0009\n",
            "Epoch 9/10, Loss: 0.0006\n",
            "Epoch 10/10, Loss: 0.0004\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I     0.9697    0.9428    0.9561       577\n",
            "           O     0.9969    0.9984    0.9977     10789\n",
            "\n",
            "    accuracy                         0.9956     11366\n",
            "   macro avg     0.9833    0.9706    0.9769     11366\n",
            "weighted avg     0.9956    0.9956    0.9956     11366\n",
            "\n",
            "Accuracy: 0.9956\n",
            "Precision: 0.9956\n",
            "Recall: 0.9956\n",
            "F1 Score: 0.9956\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IE Data (adapted for IE tagging)\n",
        "def load_excel_data_ie(file_path):\n",
        "    # Assuming same format as IO but with IE tags instead\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # Use -100 to ignore in loss\n",
        "\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # shape: (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with detailed metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # Ignore padding tokens\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IE.xlsx\"  # Your IE tagging dataset path\n",
        "    sentences, tags = load_excel_data_ie(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OQwOmmesv3X",
        "outputId": "e5f926ca-24ce-446f-9dbf-0bf9659a574a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3476\n",
            "Epoch 2/10, Loss: 0.0801\n",
            "Epoch 3/10, Loss: 0.0409\n",
            "Epoch 4/10, Loss: 0.0245\n",
            "Epoch 5/10, Loss: 0.0142\n",
            "Epoch 6/10, Loss: 0.0084\n",
            "Epoch 7/10, Loss: 0.0054\n",
            "Epoch 8/10, Loss: 0.0034\n",
            "Epoch 9/10, Loss: 0.0021\n",
            "Epoch 10/10, Loss: 0.0014\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E     0.9508    0.9280    0.9393       250\n",
            "          EO     0.9664    0.9583    0.9623       240\n",
            "           I     0.9582    0.9228    0.9402       298\n",
            "          IO     0.9957    0.9975    0.9966     10773\n",
            "\n",
            "    accuracy                         0.9933     11561\n",
            "   macro avg     0.9678    0.9517    0.9596     11561\n",
            "weighted avg     0.9932    0.9933    0.9932     11561\n",
            "\n",
            "Accuracy: 0.9933\n",
            "Precision: 0.9932\n",
            "Recall: 0.9933\n",
            "F1 Score: 0.9932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IOB Data\n",
        "def load_excel_data_iob(file_path):\n",
        "    # Assumes dataset with columns 'Word i' and 'Word i entity tag' using IOB tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IOB.xlsx\"  # Your IOB tagging dataset path\n",
        "    sentences, tags = load_excel_data_iob(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZtRlHbdtOJG",
        "outputId": "752817ae-b647-4532-f868-816474b5ba42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.2283\n",
            "Epoch 2/10, Loss: 0.0456\n",
            "Epoch 3/10, Loss: 0.0237\n",
            "Epoch 4/10, Loss: 0.0129\n",
            "Epoch 5/10, Loss: 0.0067\n",
            "Epoch 6/10, Loss: 0.0039\n",
            "Epoch 7/10, Loss: 0.0023\n",
            "Epoch 8/10, Loss: 0.0020\n",
            "Epoch 9/10, Loss: 0.0011\n",
            "Epoch 10/10, Loss: 0.0008\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B     0.9835    0.9370    0.9597       254\n",
            "           I     0.9788    0.9264    0.9519       299\n",
            "           O     0.9965    0.9991    0.9978     10744\n",
            "\n",
            "    accuracy                         0.9958     11297\n",
            "   macro avg     0.9862    0.9542    0.9698     11297\n",
            "weighted avg     0.9957    0.9958    0.9957     11297\n",
            "\n",
            "Accuracy: 0.9958\n",
            "Precision: 0.9957\n",
            "Recall: 0.9958\n",
            "F1 Score: 0.9957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IOBES Data\n",
        "def load_excel_data_iobes(file_path):\n",
        "    # Assumes dataset with columns 'Word i' and 'Word i entity tag' using IOBES tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IOBES.xlsx\"  # Your IOBES tagging dataset path\n",
        "    sentences, tags = load_excel_data_iobes(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_ThWAo3uDqL",
        "outputId": "b279be59-bf2b-4b5a-9ef3-cb2b8585db58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3241\n",
            "Epoch 2/10, Loss: 0.0641\n",
            "Epoch 3/10, Loss: 0.0300\n",
            "Epoch 4/10, Loss: 0.0184\n",
            "Epoch 5/10, Loss: 0.0128\n",
            "Epoch 6/10, Loss: 0.0083\n",
            "Epoch 7/10, Loss: 0.0054\n",
            "Epoch 8/10, Loss: 0.0038\n",
            "Epoch 9/10, Loss: 0.0026\n",
            "Epoch 10/10, Loss: 0.0019\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B     0.9960    0.9691    0.9824       259\n",
            "           E     0.9881    0.9614    0.9746       259\n",
            "           I     1.0000    0.8824    0.9375        68\n",
            "           O     0.9979    0.9998    0.9988     11210\n",
            "           S     1.0000    1.0000    1.0000         2\n",
            "\n",
            "    accuracy                         0.9976     11798\n",
            "   macro avg     0.9964    0.9625    0.9787     11798\n",
            "weighted avg     0.9976    0.9976    0.9976     11798\n",
            "\n",
            "Accuracy: 0.9976\n",
            "Precision: 0.9976\n",
            "Recall: 0.9976\n",
            "F1 Score: 0.9976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IOE Data\n",
        "def load_excel_data_ioe(file_path):\n",
        "    # Assumes dataset with columns 'Word i' and 'Word i entity tag' using IOE tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/IOE.xlsx\"  # Your IOE tagging dataset path\n",
        "    sentences, tags = load_excel_data_ioe(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZSZXTuNufyi",
        "outputId": "612f18ab-7dc3-4190-e9d1-9230623d0738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.2238\n",
            "Epoch 2/10, Loss: 0.0441\n",
            "Epoch 3/10, Loss: 0.0244\n",
            "Epoch 4/10, Loss: 0.0149\n",
            "Epoch 5/10, Loss: 0.0090\n",
            "Epoch 6/10, Loss: 0.0054\n",
            "Epoch 7/10, Loss: 0.0036\n",
            "Epoch 8/10, Loss: 0.0021\n",
            "Epoch 9/10, Loss: 0.0012\n",
            "Epoch 10/10, Loss: 0.0009\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E     0.9816    0.9175    0.9485       291\n",
            "           I     0.9819    0.9420    0.9615       345\n",
            "           O     0.9962    0.9991    0.9976     11196\n",
            "\n",
            "    accuracy                         0.9954     11832\n",
            "   macro avg     0.9866    0.9529    0.9692     11832\n",
            "weighted avg     0.9954    0.9954    0.9954     11832\n",
            "\n",
            "Accuracy: 0.9954\n",
            "Precision: 0.9954\n",
            "Recall: 0.9954\n",
            "F1 Score: 0.9954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load BI Data\n",
        "def load_excel_data_bi(file_path):\n",
        "    # Assumes dataset with columns 'Word i' and 'Word i entity tag' using BI tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/BI.xlsx\"  # Your BI tagging dataset path\n",
        "    sentences, tags = load_excel_data_bi(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bqJt7RGvLaV",
        "outputId": "a472405c-46a4-4c79-e240-e8d03b25159f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3261\n",
            "Epoch 2/10, Loss: 0.0683\n",
            "Epoch 3/10, Loss: 0.0319\n",
            "Epoch 4/10, Loss: 0.0177\n",
            "Epoch 5/10, Loss: 0.0116\n",
            "Epoch 6/10, Loss: 0.0073\n",
            "Epoch 7/10, Loss: 0.0050\n",
            "Epoch 8/10, Loss: 0.0033\n",
            "Epoch 9/10, Loss: 0.0025\n",
            "Epoch 10/10, Loss: 0.0016\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B     0.9799    0.9644    0.9721       253\n",
            "          BO     0.9686    0.9204    0.9439       201\n",
            "           I     0.9815    0.9138    0.9464       290\n",
            "          IO     0.9958    0.9989    0.9973     10882\n",
            "\n",
            "    accuracy                         0.9947     11626\n",
            "   macro avg     0.9814    0.9494    0.9649     11626\n",
            "weighted avg     0.9946    0.9947    0.9946     11626\n",
            "\n",
            "Accuracy: 0.9947\n",
            "Precision: 0.9946\n",
            "Recall: 0.9947\n",
            "F1 Score: 0.9946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load BIES Data\n",
        "def load_excel_data_bies(file_path):\n",
        "    # Dataset with columns 'Word i' and 'Word i entity tag' using BIES tagging scheme\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Build vocabularies for words and tags\n",
        "def build_vocab(sentences, tags):\n",
        "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    tag2idx = {}\n",
        "    for sent in sentences:\n",
        "        for word in sent:\n",
        "            if word not in word2idx:\n",
        "                word2idx[word] = len(word2idx)\n",
        "    for tag_seq in tags:\n",
        "        for tag in tag_seq:\n",
        "            if tag not in tag2idx:\n",
        "                tag2idx[tag] = len(tag2idx)\n",
        "    return word2idx, tag2idx\n",
        "\n",
        "# 3. Dataset class\n",
        "class NERDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, word2idx, tag2idx, max_len=50):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent = self.sentences[idx]\n",
        "        tag_seq = self.tags[idx]\n",
        "\n",
        "        # Convert words and tags to indices\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in sent]\n",
        "        tag_ids = [self.tag2idx[t] for t in tag_seq]\n",
        "\n",
        "        # Padding\n",
        "        pad_len = self.max_len - len(word_ids)\n",
        "        if pad_len > 0:\n",
        "            word_ids = word_ids + [self.word2idx['<PAD>']] * pad_len\n",
        "            tag_ids = tag_ids + [-100] * pad_len  # ignore_index for loss\n",
        "        else:\n",
        "            word_ids = word_ids[:self.max_len]\n",
        "            tag_ids = tag_ids[:self.max_len]\n",
        "\n",
        "        return torch.tensor(word_ids), torch.tensor(tag_ids)\n",
        "\n",
        "# 4. BiLSTM model for sequence tagging\n",
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, embedding_dim=100, hidden_dim=128):\n",
        "        super(BiLSTMTagger, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim*2, tagset_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(emb)\n",
        "        logits = self.fc(lstm_out)\n",
        "        return logits\n",
        "\n",
        "# 5. Training function\n",
        "def train_epoch(model, data_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for x_batch, y_batch in data_loader:\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_batch)  # (batch_size, seq_len, num_tags)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])\n",
        "        y_batch = y_batch.view(-1)\n",
        "\n",
        "        loss = criterion(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# 6. Evaluation function with classification report and metrics\n",
        "def eval_model_metrics(model, data_loader, tag2idx, device):\n",
        "    model.eval()\n",
        "    true_tags = []\n",
        "    pred_tags = []\n",
        "\n",
        "    idx2tag = {v:k for k,v in tag2idx.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x_batch, y_batch in data_loader:\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            outputs = model(x_batch)\n",
        "            preds = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "            for i in range(len(y_batch)):\n",
        "                true_seq = y_batch[i].cpu().numpy()\n",
        "                pred_seq = preds[i].cpu().numpy()\n",
        "                for t, p in zip(true_seq, pred_seq):\n",
        "                    if t != -100:  # ignore padding\n",
        "                        true_tags.append(idx2tag[t])\n",
        "                        pred_tags.append(idx2tag[p])\n",
        "\n",
        "    report = classification_report(true_tags, pred_tags, digits=4)\n",
        "    accuracy = accuracy_score(true_tags, pred_tags)\n",
        "    precision = precision_score(true_tags, pred_tags, average='weighted')\n",
        "    recall = recall_score(true_tags, pred_tags, average='weighted')\n",
        "    f1 = f1_score(true_tags, pred_tags, average='weighted')\n",
        "\n",
        "    return report, accuracy, precision, recall, f1\n",
        "\n",
        "# 7. Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"/content/BIES.xlsx\"  # Your BIES tagging dataset path\n",
        "    sentences, tags = load_excel_data_bies(file_path)\n",
        "    word2idx, tag2idx = build_vocab(sentences, tags)\n",
        "\n",
        "    max_len = 50\n",
        "    dataset = NERDataset(sentences, tags, word2idx, tag2idx, max_len=max_len)\n",
        "\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = BiLSTMTagger(len(word2idx), len(tag2idx)).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    epochs = 10\n",
        "    for epoch in range(epochs):\n",
        "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    print(\"\\nEvaluation on test data:\")\n",
        "    report, accuracy, precision, recall, f1 = eval_model_metrics(model, test_loader, tag2idx, device)\n",
        "    print(report)\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDaJHL8UvrwK",
        "outputId": "0f54756b-77df-440b-b6b5-3ba910222894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.5031\n",
            "Epoch 2/10, Loss: 0.1137\n",
            "Epoch 3/10, Loss: 0.0555\n",
            "Epoch 4/10, Loss: 0.0336\n",
            "Epoch 5/10, Loss: 0.0207\n",
            "Epoch 6/10, Loss: 0.0137\n",
            "Epoch 7/10, Loss: 0.0093\n",
            "Epoch 8/10, Loss: 0.0063\n",
            "Epoch 9/10, Loss: 0.0045\n",
            "Epoch 10/10, Loss: 0.0034\n",
            "\n",
            "Evaluation on test data:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B     0.9922    0.9410    0.9659       271\n",
            "          BO     0.9559    0.9330    0.9443       209\n",
            "           E     0.9762    0.9077    0.9407       271\n",
            "          EO     0.9801    0.9535    0.9666       258\n",
            "           I     0.9565    0.7097    0.8148        62\n",
            "          IO     0.9922    0.9986    0.9954     10290\n",
            "           S     0.0000    0.0000    0.0000         2\n",
            "          SO     1.0000    0.3333    0.5000         6\n",
            "\n",
            "    accuracy                         0.9908     11369\n",
            "   macro avg     0.8566    0.7221    0.7660     11369\n",
            "weighted avg     0.9905    0.9908    0.9904     11369\n",
            "\n",
            "Accuracy: 0.9908\n",
            "Precision: 0.9905\n",
            "Recall: 0.9908\n",
            "F1 Score: 0.9904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sPQSXbIWvsBv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}