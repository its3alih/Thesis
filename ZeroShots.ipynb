{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMrwuDJvQ3+diqalV1l91I3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its3alih/Thesis/blob/main/ZeroShots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FIRST"
      ],
      "metadata": {
        "id": "8JgOIQmT3pql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets seqeval openpyxl\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnAm0gnu1z_J",
        "outputId": "d28714bd-1719-48a9-85d6-bc1ba1080029",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python --quiet\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3NJDhll326v8",
        "outputId": "3bffbb17-87e3-4ed3-993b-6c5256424f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZckJ2DT4Voy",
        "outputId": "8830bc09-735a-4548-e029-35fab4c650b0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf -O tinyllama.gguf\n"
      ],
      "metadata": {
        "id": "K1b1t4Kl6x34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-bies\", name=\"zero-shot-mistral\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # Set to >0 if you're using Colab Pro with T4/A100\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load BIES-tagged Excel file\n",
        "df = pd.read_excel(\"/content/BIES.xlsx\")\n",
        "\n",
        "# Filter valid BIES tags\n",
        "valid_tags = [\"B\", \"I\", \"E\", \"S\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define prompt\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the word below into one of these entity tags:\\n\"\n",
        "        f\"B = Beginning of an entity\\n\"\n",
        "        f\"I = Inside an entity\\n\"\n",
        "        f\"E = End of an entity\\n\"\n",
        "        f\"S = Single-word entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Inference and prediction\n",
        "n_test = 10  # You can increase this\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log prediction to wandb table\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions to wandb\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Log metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "# STEP 10: Log metrics to wandb\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 11: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SHQIsosKB7BL",
        "outputId": "d04f0774-eb54-4b07-f074-5c8aab3472f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_231651-hkiy0qyw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/hkiy0qyw' target=\"_blank\">zero-shot-mistral</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/hkiy0qyw' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/hkiy0qyw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B      0.000     0.000     0.000         3\n",
            "           E      0.250     0.500     0.333         2\n",
            "           I      0.500     0.500     0.500         2\n",
            "           O      0.000     0.000     0.000         0\n",
            "           S      0.000     0.000     0.000         3\n",
            "\n",
            "    accuracy                          0.200        10\n",
            "   macro avg      0.150     0.200     0.167        10\n",
            "weighted avg      0.150     0.200     0.167        10\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.2000\n",
            "Macro Precision:   0.1500\n",
            "Macro Recall:      0.2000\n",
            "Macro F1 Score:    0.1667\n",
            "Weighted F1 Score: 0.1667\n",
            "Micro F1 Score:    0.2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.2</td></tr><tr><td>macro_f1</td><td>0.16667</td></tr><tr><td>macro_precision</td><td>0.15</td></tr><tr><td>macro_recall</td><td>0.2</td></tr><tr><td>micro_f1</td><td>0.2</td></tr><tr><td>weighted_f1</td><td>0.16667</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-mistral</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/hkiy0qyw' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/hkiy0qyw</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_231651-hkiy0qyw/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-bi\", name=\"zero-shot-tinyllama-bi\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Make sure this file exists in your runtime\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # >0 for faster inference with GPU\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load BI-tagged Excel file\n",
        "df = pd.read_excel(\"/content/BI.xlsx\")  # Change to the correct path if needed\n",
        "\n",
        "# Keep only valid BI tags\n",
        "valid_tags = [\"B\", \"I\", \"O\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Keep necessary columns\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define the prompt function\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the word below into one of these entity tags:\\n\"\n",
        "        f\"B = Beginning of an entity\\n\"\n",
        "        f\"I = Inside of an entity\\n\"\n",
        "        f\"O = Outside (not part of an entity)\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Run inference and collect predictions\n",
        "n_test = 20  # You can increase this\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log prediction to wandb table\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions to wandb\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Log metrics to wandb\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 10: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_CSzj6EZ7_BD",
        "outputId": "1b23730c-e9f7-48ed-ab30-a8d983895f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_225041-8chdpajz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/8chdpajz' target=\"_blank\">zero-shot-tinyllama-bi</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/8chdpajz' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/8chdpajz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B      0.333     0.111     0.167         9\n",
            "           I      0.600     0.273     0.375        11\n",
            "           O      0.000     0.000     0.000         0\n",
            "\n",
            "    accuracy                          0.200        20\n",
            "   macro avg      0.311     0.128     0.181        20\n",
            "weighted avg      0.480     0.200     0.281        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.2000\n",
            "Macro Precision:   0.3111\n",
            "Macro Recall:      0.1279\n",
            "Macro F1 Score:    0.1806\n",
            "Weighted F1 Score: 0.2812\n",
            "Micro F1 Score:    0.2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.2</td></tr><tr><td>macro_f1</td><td>0.18056</td></tr><tr><td>macro_precision</td><td>0.31111</td></tr><tr><td>macro_recall</td><td>0.12795</td></tr><tr><td>micro_f1</td><td>0.2</td></tr><tr><td>weighted_f1</td><td>0.28125</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-bi</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/8chdpajz' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/8chdpajz</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_225041-8chdpajz/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-ie\", name=\"zero-shot-tinyllama-ie\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Ensure this file exists in your runtime\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # Set to >0 if using GPU (e.g., Colab Pro)\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IE-tagged Excel file\n",
        "df = pd.read_excel(\"/content/IE.xlsx\")  # Replace with the correct path if needed\n",
        "\n",
        "# Keep only valid IE tags\n",
        "valid_tags = [\"I\", \"E\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Select necessary columns\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define the prompt function\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the word below into one of these entity tags:\\n\"\n",
        "        f\"I = Inside of an entity\\n\"\n",
        "        f\"E = End of an entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Run inference and collect predictions\n",
        "n_test = 20  # Adjust as needed\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log prediction to wandb table\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions to wandb\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Log metrics to wandb\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 10: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JH0q91808mBH",
        "outputId": "e501bb18-f336-4d17-b690-d0619f22d541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_225346-hj1mxsn5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/hj1mxsn5' target=\"_blank\">zero-shot-tinyllama-ie</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/hj1mxsn5' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/hj1mxsn5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E      0.250     0.250     0.250         8\n",
            "           I      0.500     0.083     0.143        12\n",
            "           O      0.000     0.000     0.000         0\n",
            "\n",
            "    accuracy                          0.150        20\n",
            "   macro avg      0.250     0.111     0.131        20\n",
            "weighted avg      0.400     0.150     0.186        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.1500\n",
            "Macro Precision:   0.2500\n",
            "Macro Recall:      0.1111\n",
            "Macro F1 Score:    0.1310\n",
            "Weighted F1 Score: 0.1857\n",
            "Micro F1 Score:    0.1500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.15</td></tr><tr><td>macro_f1</td><td>0.13095</td></tr><tr><td>macro_precision</td><td>0.25</td></tr><tr><td>macro_recall</td><td>0.11111</td></tr><tr><td>micro_f1</td><td>0.15</td></tr><tr><td>weighted_f1</td><td>0.18571</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-ie</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/hj1mxsn5' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/hj1mxsn5</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_225346-hj1mxsn5/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-iob\", name=\"zero-shot-tinyllama-iob\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Path to your .gguf file\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # Set >0 if using GPU (Colab Pro, etc.)\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IOB-tagged Excel file\n",
        "df = pd.read_excel(\"/content/IOB.xlsx\")  # Update this path as needed\n",
        "\n",
        "# Keep only valid IOB tags\n",
        "valid_tags = [\"I\", \"O\", \"B\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Select the necessary columns\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define the prompt template\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the word below into one of these entity tags:\\n\"\n",
        "        f\"B = Beginning of an entity\\n\"\n",
        "        f\"I = Inside of an entity\\n\"\n",
        "        f\"O = Outside any entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Inference\n",
        "n_test = 20  # Adjust as needed\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log to wandb\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Log metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 10: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "PJMOlkQd-gT3",
        "outputId": "ba13f39b-1e76-4612-a33b-ce513b9144a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_230142-uwjm7ukd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/uwjm7ukd' target=\"_blank\">zero-shot-tinyllama-iob</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/uwjm7ukd' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/uwjm7ukd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B      0.000     0.000     0.000         3\n",
            "           I      0.400     0.500     0.444         4\n",
            "           O      0.692     0.692     0.692        13\n",
            "\n",
            "    accuracy                          0.550        20\n",
            "   macro avg      0.364     0.397     0.379        20\n",
            "weighted avg      0.530     0.550     0.539        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.5500\n",
            "Macro Precision:   0.3641\n",
            "Macro Recall:      0.3974\n",
            "Macro F1 Score:    0.3789\n",
            "Weighted F1 Score: 0.5389\n",
            "Micro F1 Score:    0.5500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.55</td></tr><tr><td>macro_f1</td><td>0.37892</td></tr><tr><td>macro_precision</td><td>0.3641</td></tr><tr><td>macro_recall</td><td>0.39744</td></tr><tr><td>micro_f1</td><td>0.55</td></tr><tr><td>weighted_f1</td><td>0.53889</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-iob</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/uwjm7ukd' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/uwjm7ukd</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_230142-uwjm7ukd/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-iobes\", name=\"zero-shot-tinyllama-iobes\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Path to your local GGUF model\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # Use >0 if you have GPU\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IOBES-tagged Excel file\n",
        "df = pd.read_excel(\"/content/IOBES.xlsx\")  # Update this if your file path is different\n",
        "\n",
        "# Valid IOBES tags\n",
        "valid_tags = [\"I\", \"O\", \"B\", \"E\", \"S\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Keep necessary columns\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define prompt template\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the following word using one of these entity tags:\\n\"\n",
        "        f\"B = Beginning of an entity\\n\"\n",
        "        f\"I = Inside an entity\\n\"\n",
        "        f\"O = Outside any entity\\n\"\n",
        "        f\"E = End of a multi-word entity\\n\"\n",
        "        f\"S = Single-word entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Inference\n",
        "n_test = 20  # Adjust for more or less test samples\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log to wandb\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True label\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions to wandb\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "# STEP 10: Log metrics\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 11: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gVCqMK3y-8eL",
        "outputId": "46ddafda-be18-4134-ae0c-d8f61624e3b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_230348-ha2xmii7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/ha2xmii7' target=\"_blank\">zero-shot-tinyllama-iobes</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/ha2xmii7' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/ha2xmii7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B      1.000     0.500     0.667         2\n",
            "           E      0.250     0.500     0.333         2\n",
            "           I      0.333     0.500     0.400         2\n",
            "           O      0.667     0.615     0.640        13\n",
            "           S      0.000     0.000     0.000         1\n",
            "\n",
            "    accuracy                          0.550        20\n",
            "   macro avg      0.450     0.423     0.408        20\n",
            "weighted avg      0.592     0.550     0.556        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.5500\n",
            "Macro Precision:   0.4500\n",
            "Macro Recall:      0.4231\n",
            "Macro F1 Score:    0.4080\n",
            "Weighted F1 Score: 0.5560\n",
            "Micro F1 Score:    0.5500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.55</td></tr><tr><td>macro_f1</td><td>0.408</td></tr><tr><td>macro_precision</td><td>0.45</td></tr><tr><td>macro_recall</td><td>0.42308</td></tr><tr><td>micro_f1</td><td>0.55</td></tr><tr><td>weighted_f1</td><td>0.556</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-iobes</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/ha2xmii7' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/ha2xmii7</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_230348-ha2xmii7/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-ioe\", name=\"zero-shot-tinyllama-ioe\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Local GGUF model\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IOE-tagged dataset\n",
        "df = pd.read_excel(\"/content/IOE.xlsx\")  # Replace with correct path\n",
        "\n",
        "# Filter for valid IOE tags\n",
        "valid_tags = [\"I\", \"O\", \"E\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Select relevant features\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Prompt generator\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the following word using one of these entity tags:\\n\"\n",
        "        f\"I = Inside an entity\\n\"\n",
        "        f\"O = Outside any entity\\n\"\n",
        "        f\"E = End of an entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Run inference\n",
        "n_test = 20  # Sample size for testing\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log prediction\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions table\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluate\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Calculate metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "# STEP 10: Log metrics\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 11: Finish run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DN1yDflCAB3o",
        "outputId": "61d48d08-cc75-42e0-fe48-f1803463dcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_230822-ubaijo1q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/ubaijo1q' target=\"_blank\">zero-shot-tinyllama-ioe</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/ubaijo1q' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/ubaijo1q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E      0.000     0.000     0.000         3\n",
            "           I      0.000     0.000     0.000         4\n",
            "           O      0.688     0.846     0.759        13\n",
            "\n",
            "    accuracy                          0.550        20\n",
            "   macro avg      0.229     0.282     0.253        20\n",
            "weighted avg      0.447     0.550     0.493        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.5500\n",
            "Macro Precision:   0.2292\n",
            "Macro Recall:      0.2821\n",
            "Macro F1 Score:    0.2529\n",
            "Weighted F1 Score: 0.4931\n",
            "Micro F1 Score:    0.5500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.55</td></tr><tr><td>macro_f1</td><td>0.25287</td></tr><tr><td>macro_precision</td><td>0.22917</td></tr><tr><td>macro_recall</td><td>0.28205</td></tr><tr><td>micro_f1</td><td>0.55</td></tr><tr><td>weighted_f1</td><td>0.4931</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-ioe</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/ubaijo1q' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/ubaijo1q</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_230822-ubaijo1q/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-io\", name=\"zero-shot-tinyllama-io\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Make sure this file exists in your Colab or local runtime\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IO-tagged dataset\n",
        "df = pd.read_excel(\"/content/IO.xlsx\")  # Replace with actual path\n",
        "\n",
        "# Filter for valid IO tags\n",
        "valid_tags = [\"I\", \"O\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Select relevant features\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Prompt generator\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the following word using one of these entity tags:\\n\"\n",
        "        f\"I = Inside an entity\\n\"\n",
        "        f\"O = Outside any entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Inference\n",
        "n_test = 20  # Adjust for more test samples\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log to wandb table\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word\n",
        "        row._2,  # True label\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Calculate metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "# STEP 10: Log metrics\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 11: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "oQ0NGT2LAeTj",
        "outputId": "ec04c9c7-64e5-463a-c498-31448425461a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_231027-9wewwnfi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/9wewwnfi' target=\"_blank\">zero-shot-tinyllama-io</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/9wewwnfi' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/9wewwnfi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I      0.500     0.143     0.222         7\n",
            "           O      0.667     0.923     0.774        13\n",
            "\n",
            "    accuracy                          0.650        20\n",
            "   macro avg      0.583     0.533     0.498        20\n",
            "weighted avg      0.608     0.650     0.581        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.6500\n",
            "Macro Precision:   0.5833\n",
            "Macro Recall:      0.5330\n",
            "Macro F1 Score:    0.4982\n",
            "Weighted F1 Score: 0.5810\n",
            "Micro F1 Score:    0.6500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.65</td></tr><tr><td>macro_f1</td><td>0.49821</td></tr><tr><td>macro_precision</td><td>0.58333</td></tr><tr><td>macro_recall</td><td>0.53297</td></tr><tr><td>micro_f1</td><td>0.65</td></tr><tr><td>weighted_f1</td><td>0.581</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-io</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/9wewwnfi' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/9wewwnfi</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_231027-9wewwnfi/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SECOND"
      ],
      "metadata": {
        "id": "2bmcfvB_Uhwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-bies\", name=\"zero-shot-mistral\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # Set to >0 if you're using Colab Pro with T4/A100\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load BIES-tagged Excel file\n",
        "df = pd.read_excel(\"/content/BIES2.xlsx\")\n",
        "\n",
        "# Filter valid BIES tags\n",
        "valid_tags = [\"B\", \"I\", \"E\", \"S\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define prompt\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the word below into one of these entity tags:\\n\"\n",
        "        f\"B = Beginning of an entity\\n\"\n",
        "        f\"I = Inside an entity\\n\"\n",
        "        f\"E = End of an entity\\n\"\n",
        "        f\"S = Single-word entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Inference and prediction\n",
        "n_test = 10  # You can increase this\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log prediction to wandb table\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions to wandb\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Log metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "# STEP 10: Log metrics to wandb\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 11: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qqhOjYzvCjAX",
        "outputId": "ff5d6a4f-1f1c-416d-a92b-82a9ee9594c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_232154-dglappik</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/dglappik' target=\"_blank\">zero-shot-mistral</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/dglappik' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/dglappik</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B      0.000     0.000     0.000         3\n",
            "           E      0.250     0.500     0.333         2\n",
            "           I      0.500     0.500     0.500         2\n",
            "           O      0.000     0.000     0.000         0\n",
            "           S      0.000     0.000     0.000         3\n",
            "\n",
            "    accuracy                          0.200        10\n",
            "   macro avg      0.150     0.200     0.167        10\n",
            "weighted avg      0.150     0.200     0.167        10\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.2000\n",
            "Macro Precision:   0.1500\n",
            "Macro Recall:      0.2000\n",
            "Macro F1 Score:    0.1667\n",
            "Weighted F1 Score: 0.1667\n",
            "Micro F1 Score:    0.2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.2</td></tr><tr><td>macro_f1</td><td>0.16667</td></tr><tr><td>macro_precision</td><td>0.15</td></tr><tr><td>macro_recall</td><td>0.2</td></tr><tr><td>micro_f1</td><td>0.2</td></tr><tr><td>weighted_f1</td><td>0.16667</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-mistral</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/dglappik' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies/runs/dglappik</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bies</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_232154-dglappik/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-bi\", name=\"zero-shot-tinyllama-bi\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Make sure this file exists in your runtime\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # >0 for faster inference with GPU\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load BI-tagged Excel file\n",
        "df = pd.read_excel(\"/content/BI2.xlsx\")  # Change to the correct path if needed\n",
        "\n",
        "# Keep only valid BI tags\n",
        "valid_tags = [\"B\", \"I\", \"O\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Keep necessary columns\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define the prompt function\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the word below into one of these entity tags:\\n\"\n",
        "        f\"B = Beginning of an entity\\n\"\n",
        "        f\"I = Inside of an entity\\n\"\n",
        "        f\"O = Outside (not part of an entity)\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Run inference and collect predictions\n",
        "n_test = 20  # You can increase this\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log prediction to wandb table\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions to wandb\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Log metrics to wandb\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 10: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "me39Yav7DmEF",
        "outputId": "32fddf41-4ea7-4981-d664-ca536715f048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_232359-lhbxln5y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/lhbxln5y' target=\"_blank\">zero-shot-tinyllama-bi</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/lhbxln5y' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/lhbxln5y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B      0.667     0.333     0.444        12\n",
            "           I      0.400     0.250     0.308         8\n",
            "           O      0.000     0.000     0.000         0\n",
            "\n",
            "    accuracy                          0.300        20\n",
            "   macro avg      0.356     0.194     0.251        20\n",
            "weighted avg      0.560     0.300     0.390        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.3000\n",
            "Macro Precision:   0.3556\n",
            "Macro Recall:      0.1944\n",
            "Macro F1 Score:    0.2507\n",
            "Weighted F1 Score: 0.3897\n",
            "Micro F1 Score:    0.3000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.3</td></tr><tr><td>macro_f1</td><td>0.25071</td></tr><tr><td>macro_precision</td><td>0.35556</td></tr><tr><td>macro_recall</td><td>0.19444</td></tr><tr><td>micro_f1</td><td>0.3</td></tr><tr><td>weighted_f1</td><td>0.38974</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-bi</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/lhbxln5y' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi/runs/lhbxln5y</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-bi</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_232359-lhbxln5y/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-ie\", name=\"zero-shot-tinyllama-ie\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Ensure this file exists in your runtime\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # Set to >0 if using GPU (e.g., Colab Pro)\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IE-tagged Excel file\n",
        "df = pd.read_excel(\"/content/IE2.xlsx\")  # Replace with the correct path if needed\n",
        "\n",
        "# Keep only valid IE tags\n",
        "valid_tags = [\"I\", \"E\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Select necessary columns\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define the prompt function\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the word below into one of these entity tags:\\n\"\n",
        "        f\"I = Inside of an entity\\n\"\n",
        "        f\"E = End of an entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Run inference and collect predictions\n",
        "n_test = 20  # Adjust as needed\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log prediction to wandb table\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions to wandb\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Log metrics to wandb\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 10: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nRftVQS5Drbo",
        "outputId": "2c66a1c1-1e26-4ec0-8b54-15a0d74275dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_232608-mzvcbjj6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/mzvcbjj6' target=\"_blank\">zero-shot-tinyllama-ie</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/mzvcbjj6' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/mzvcbjj6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E      0.500     0.333     0.400        12\n",
            "           I      0.500     0.125     0.200         8\n",
            "           O      0.000     0.000     0.000         0\n",
            "\n",
            "    accuracy                          0.250        20\n",
            "   macro avg      0.333     0.153     0.200        20\n",
            "weighted avg      0.500     0.250     0.320        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.2500\n",
            "Macro Precision:   0.3333\n",
            "Macro Recall:      0.1528\n",
            "Macro F1 Score:    0.2000\n",
            "Weighted F1 Score: 0.3200\n",
            "Micro F1 Score:    0.2500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.25</td></tr><tr><td>macro_f1</td><td>0.2</td></tr><tr><td>macro_precision</td><td>0.33333</td></tr><tr><td>macro_recall</td><td>0.15278</td></tr><tr><td>micro_f1</td><td>0.25</td></tr><tr><td>weighted_f1</td><td>0.32</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-ie</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/mzvcbjj6' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie/runs/mzvcbjj6</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ie</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_232608-mzvcbjj6/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-iob\", name=\"zero-shot-tinyllama-iob\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Path to your .gguf file\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # Set >0 if using GPU (Colab Pro, etc.)\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IOB-tagged Excel file\n",
        "df = pd.read_excel(\"/content/IOB2.xlsx\")  # Update this path as needed\n",
        "\n",
        "# Keep only valid IOB tags\n",
        "valid_tags = [\"I\", \"O\", \"B\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Select the necessary columns\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define the prompt template\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the word below into one of these entity tags:\\n\"\n",
        "        f\"B = Beginning of an entity\\n\"\n",
        "        f\"I = Inside of an entity\\n\"\n",
        "        f\"O = Outside any entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Inference\n",
        "n_test = 20  # Adjust as needed\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log to wandb\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Log metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 10: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "cDlTdjaGDxSY",
        "outputId": "835902d8-b112-4b78-e7ab-667ac0966057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_232752-1zq13do0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/1zq13do0' target=\"_blank\">zero-shot-tinyllama-iob</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/1zq13do0' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/1zq13do0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B      0.000     0.000     0.000         3\n",
            "           I      0.400     0.500     0.444         4\n",
            "           O      0.692     0.692     0.692        13\n",
            "\n",
            "    accuracy                          0.550        20\n",
            "   macro avg      0.364     0.397     0.379        20\n",
            "weighted avg      0.530     0.550     0.539        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.5500\n",
            "Macro Precision:   0.3641\n",
            "Macro Recall:      0.3974\n",
            "Macro F1 Score:    0.3789\n",
            "Weighted F1 Score: 0.5389\n",
            "Micro F1 Score:    0.5500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.55</td></tr><tr><td>macro_f1</td><td>0.37892</td></tr><tr><td>macro_precision</td><td>0.3641</td></tr><tr><td>macro_recall</td><td>0.39744</td></tr><tr><td>micro_f1</td><td>0.55</td></tr><tr><td>weighted_f1</td><td>0.53889</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-iob</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/1zq13do0' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob/runs/1zq13do0</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iob</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_232752-1zq13do0/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-iobes\", name=\"zero-shot-tinyllama-iobes\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model (adjust model path as needed)\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Path to your local GGUF model\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,  # Use >0 if you have GPU\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IOBES-tagged Excel file\n",
        "df = pd.read_excel(\"/content/IOBES2.xlsx\")  # Update this if your file path is different\n",
        "\n",
        "# Valid IOBES tags\n",
        "valid_tags = [\"I\", \"O\", \"B\", \"E\", \"S\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Keep necessary columns\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Define prompt template\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the following word using one of these entity tags:\\n\"\n",
        "        f\"B = Beginning of an entity\\n\"\n",
        "        f\"I = Inside an entity\\n\"\n",
        "        f\"O = Outside any entity\\n\"\n",
        "        f\"E = End of a multi-word entity\\n\"\n",
        "        f\"S = Single-word entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Inference\n",
        "n_test = 20  # Adjust for more or less test samples\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log to wandb\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word i\n",
        "        row._2,  # True label\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions to wandb\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "# STEP 10: Log metrics\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 11: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lb7c4FxhD1jZ",
        "outputId": "ac0b9230-8ec8-464d-d0f0-2d74a2c56d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_232952-u07aiayj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/u07aiayj' target=\"_blank\">zero-shot-tinyllama-iobes</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/u07aiayj' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/u07aiayj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B      1.000     0.500     0.667         2\n",
            "           E      0.250     0.500     0.333         2\n",
            "           I      0.333     0.500     0.400         2\n",
            "           O      0.667     0.615     0.640        13\n",
            "           S      0.000     0.000     0.000         1\n",
            "\n",
            "    accuracy                          0.550        20\n",
            "   macro avg      0.450     0.423     0.408        20\n",
            "weighted avg      0.592     0.550     0.556        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.5500\n",
            "Macro Precision:   0.4500\n",
            "Macro Recall:      0.4231\n",
            "Macro F1 Score:    0.4080\n",
            "Weighted F1 Score: 0.5560\n",
            "Micro F1 Score:    0.5500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.55</td></tr><tr><td>macro_f1</td><td>0.408</td></tr><tr><td>macro_precision</td><td>0.45</td></tr><tr><td>macro_recall</td><td>0.42308</td></tr><tr><td>micro_f1</td><td>0.55</td></tr><tr><td>weighted_f1</td><td>0.556</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-iobes</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/u07aiayj' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes/runs/u07aiayj</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-iobes</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_232952-u07aiayj/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-ioe\", name=\"zero-shot-tinyllama-ioe\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Local GGUF model\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IOE-tagged dataset\n",
        "df = pd.read_excel(\"/content/IOE2.xlsx\")  # Replace with correct path\n",
        "\n",
        "# Filter for valid IOE tags\n",
        "valid_tags = [\"I\", \"O\", \"E\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Select relevant features\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Prompt generator\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the following word using one of these entity tags:\\n\"\n",
        "        f\"I = Inside an entity\\n\"\n",
        "        f\"O = Outside any entity\\n\"\n",
        "        f\"E = End of an entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Run inference\n",
        "n_test = 20  # Sample size for testing\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log prediction\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word\n",
        "        row._2,  # True tag\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions table\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluate\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Calculate metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "# STEP 10: Log metrics\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 11: Finish run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "h7icGkKsD82b",
        "outputId": "f20fb8fe-26a6-4865-d792-f04c26c048b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_233133-p0wlmucd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/p0wlmucd' target=\"_blank\">zero-shot-tinyllama-ioe</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/p0wlmucd' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/p0wlmucd</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E      0.000     0.000     0.000         3\n",
            "           I      0.000     0.000     0.000         4\n",
            "           O      0.688     0.846     0.759        13\n",
            "\n",
            "    accuracy                          0.550        20\n",
            "   macro avg      0.229     0.282     0.253        20\n",
            "weighted avg      0.447     0.550     0.493        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.5500\n",
            "Macro Precision:   0.2292\n",
            "Macro Recall:      0.2821\n",
            "Macro F1 Score:    0.2529\n",
            "Weighted F1 Score: 0.4931\n",
            "Micro F1 Score:    0.5500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.55</td></tr><tr><td>macro_f1</td><td>0.25287</td></tr><tr><td>macro_precision</td><td>0.22917</td></tr><tr><td>macro_recall</td><td>0.28205</td></tr><tr><td>micro_f1</td><td>0.55</td></tr><tr><td>weighted_f1</td><td>0.4931</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-ioe</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/p0wlmucd' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe/runs/p0wlmucd</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-ioe</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_233133-p0wlmucd/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install llama-cpp-python wandb pandas scikit-learn openpyxl --quiet\n",
        "\n",
        "# STEP 2: Import libraries\n",
        "from llama_cpp import Llama\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "import wandb\n",
        "\n",
        "# STEP 3: Initialize wandb\n",
        "wandb.init(project=\"arabic-ner-io\", name=\"zero-shot-tinyllama-io\", reinit=True)\n",
        "\n",
        "# STEP 4: Load model\n",
        "llm = Llama(\n",
        "    model_path=\"tinyllama.gguf\",  # Make sure this file exists in your Colab or local runtime\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=0,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# STEP 5: Load IO-tagged dataset\n",
        "df = pd.read_excel(\"/content/IO2.xlsx\")  # Replace with actual path\n",
        "\n",
        "# Filter for valid IO tags\n",
        "valid_tags = [\"I\", \"O\"]\n",
        "df = df[df[\"Word i entity tag\"].isin(valid_tags)]\n",
        "\n",
        "# Select relevant features\n",
        "columns = [\n",
        "    \"Word i\", \"Word i entity tag\", \"Word i POS\", \"Stopword\",\n",
        "    \"Word i Gazetteers\", \"Word i Lexical marker\", \"Word i definiteness\"\n",
        "]\n",
        "df = df[columns].reset_index(drop=True)\n",
        "\n",
        "# STEP 6: Prompt generator\n",
        "def make_prompt(row):\n",
        "    return (\n",
        "        f\"You are a medical NLP expert.\\n\"\n",
        "        f\"Classify the following word using one of these entity tags:\\n\"\n",
        "        f\"I = Inside an entity\\n\"\n",
        "        f\"O = Outside any entity\\n\\n\"\n",
        "        f\"Word: {row['Word i']}\\n\"\n",
        "        f\"POS: {row['Word i POS']}\\n\"\n",
        "        f\"Stopword: {row['Stopword']}\\n\"\n",
        "        f\"Gazetteer: {row['Word i Gazetteers']}\\n\"\n",
        "        f\"Lexical marker: {row['Word i Lexical marker']}\\n\"\n",
        "        f\"Definiteness: {row['Word i definiteness']}\\n\"\n",
        "        f\"Entity Tag:\"\n",
        "    )\n",
        "\n",
        "# STEP 7: Inference\n",
        "n_test = 20  # Adjust for more test samples\n",
        "test_rows = df.iloc[:n_test]\n",
        "prompts = [make_prompt(row) for _, row in test_rows.iterrows()]\n",
        "true_labels = test_rows[\"Word i entity tag\"].tolist()\n",
        "\n",
        "predicted_labels = []\n",
        "wandb_table = wandb.Table(columns=[\"Index\", \"Word\", \"True Label\", \"Predicted Label\", \"Prompt\", \"Model Output\"])\n",
        "\n",
        "for idx, (prompt, row) in enumerate(zip(prompts, test_rows.itertuples())):\n",
        "    response = llm(prompt, max_tokens=10)\n",
        "    text = response[\"choices\"][0][\"text\"].strip()\n",
        "    tag = text.split()[0].upper()\n",
        "    prediction = tag if tag in valid_tags else \"O\"\n",
        "    predicted_labels.append(prediction)\n",
        "\n",
        "    # Log to wandb table\n",
        "    wandb_table.add_data(\n",
        "        idx,\n",
        "        row._1,  # Word\n",
        "        row._2,  # True label\n",
        "        prediction,\n",
        "        prompt,\n",
        "        text\n",
        "    )\n",
        "\n",
        "# STEP 8: Log predictions\n",
        "wandb.log({\"predictions_table\": wandb_table})\n",
        "\n",
        "# STEP 9: Evaluation\n",
        "print(\"\\nüìä Classification Report:\\n\")\n",
        "print(classification_report(true_labels, predicted_labels, digits=3))\n",
        "\n",
        "# Calculate metrics\n",
        "acc = accuracy_score(true_labels, predicted_labels)\n",
        "macro_p = precision_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_r = recall_score(true_labels, predicted_labels, average=\"macro\")\n",
        "macro_f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
        "weighted_f1 = f1_score(true_labels, predicted_labels, average=\"weighted\")\n",
        "micro_f1 = f1_score(true_labels, predicted_labels, average=\"micro\")\n",
        "\n",
        "print(\"\\nüìà Evaluation Summary:\")\n",
        "print(f\"Accuracy:          {acc:.4f}\")\n",
        "print(f\"Macro Precision:   {macro_p:.4f}\")\n",
        "print(f\"Macro Recall:      {macro_r:.4f}\")\n",
        "print(f\"Macro F1 Score:    {macro_f1:.4f}\")\n",
        "print(f\"Weighted F1 Score: {weighted_f1:.4f}\")\n",
        "print(f\"Micro F1 Score:    {micro_f1:.4f}\")\n",
        "\n",
        "# STEP 10: Log metrics\n",
        "wandb.log({\n",
        "    \"accuracy\": acc,\n",
        "    \"macro_precision\": macro_p,\n",
        "    \"macro_recall\": macro_r,\n",
        "    \"macro_f1\": macro_f1,\n",
        "    \"weighted_f1\": weighted_f1,\n",
        "    \"micro_f1\": micro_f1\n",
        "})\n",
        "\n",
        "# STEP 11: Finish wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "-A-A4-gJEBxY",
        "outputId": "c5027a06-ba2b-43ac-cfc7-951d62500486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250616_233305-u5eok02p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/u5eok02p' target=\"_blank\">zero-shot-tinyllama-io</a></strong> to <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/u5eok02p' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/u5eok02p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I      0.500     0.143     0.222         7\n",
            "           O      0.667     0.923     0.774        13\n",
            "\n",
            "    accuracy                          0.650        20\n",
            "   macro avg      0.583     0.533     0.498        20\n",
            "weighted avg      0.608     0.650     0.581        20\n",
            "\n",
            "\n",
            "üìà Evaluation Summary:\n",
            "Accuracy:          0.6500\n",
            "Macro Precision:   0.5833\n",
            "Macro Recall:      0.5330\n",
            "Macro F1 Score:    0.4982\n",
            "Weighted F1 Score: 0.5810\n",
            "Micro F1 Score:    0.6500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ</td></tr><tr><td>macro_f1</td><td>‚ñÅ</td></tr><tr><td>macro_precision</td><td>‚ñÅ</td></tr><tr><td>macro_recall</td><td>‚ñÅ</td></tr><tr><td>micro_f1</td><td>‚ñÅ</td></tr><tr><td>weighted_f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.65</td></tr><tr><td>macro_f1</td><td>0.49821</td></tr><tr><td>macro_precision</td><td>0.58333</td></tr><tr><td>macro_recall</td><td>0.53297</td></tr><tr><td>micro_f1</td><td>0.65</td></tr><tr><td>weighted_f1</td><td>0.581</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">zero-shot-tinyllama-io</strong> at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/u5eok02p' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io/runs/u5eok02p</a><br> View project at: <a href='https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io' target=\"_blank\">https://wandb.ai/is-it-ali03-german-university-in-cairo/arabic-ner-io</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250616_233305-u5eok02p/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}