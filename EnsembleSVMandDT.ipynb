{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTRXPIIblXaMTsS3RIpbeT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its3alih/Thesis/blob/main/EnsembleSVMandDT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FIRST"
      ],
      "metadata": {
        "id": "8yk8fV0GwLa2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd-k7kScJCnu",
        "outputId": "405b371f-6df2-4efe-8f2f-27316d9c2749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.93      0.96      0.95       548\n",
            "           O       1.00      1.00      1.00     11201\n",
            "\n",
            "    accuracy                           1.00     11749\n",
            "   macro avg       0.97      0.98      0.97     11749\n",
            "weighted avg       1.00      1.00      1.00     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9951\n",
            "Precision: 0.9952\n",
            "Recall: 0.9951\n",
            "F1 Score: 0.9951\n"
          ]
        }
      ],
      "source": [
        "# Ensemble of SVM and Decision Tree using VotingClassifier\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load Excel Data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # End of sentence\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Feature extraction per token\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare dataset\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# 4. Load and split data\n",
        "sentences, tags = load_excel_data(\"/content/IO.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Vectorize features\n",
        "vec = DictVectorizer(sparse=True)\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# 6. Encode labels\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "\n",
        "# 7. Define base classifiers\n",
        "svm = LinearSVC(random_state=42)\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 8. Ensemble via hard voting\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('svm', svm),\n",
        "    ('dt', dt)\n",
        "], voting='hard')\n",
        "\n",
        "# 9. Fit and predict\n",
        "ensemble.fit(X_train_vec, y_train_enc)\n",
        "y_pred_enc = ensemble.predict(X_test_vec)\n",
        "y_pred = le.inverse_transform(y_pred_enc)\n",
        "\n",
        "# 10. Evaluate\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Load Excel Data with IOB Tags\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # End of sentence\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract features per token\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare token-level data\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# Load data\n",
        "sentences, tags = load_excel_data(\"/content/IOB.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode labels (VotingClassifier doesn't support string labels directly)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_test_enc = label_encoder.transform(y_test)\n",
        "\n",
        "# Vectorizer\n",
        "vec = DictVectorizer(sparse=True)\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# Base classifiers\n",
        "svm = LinearSVC()\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Voting Classifier (Hard voting)\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('svm', svm),\n",
        "    ('tree', tree)\n",
        "], voting='hard')\n",
        "\n",
        "# Train\n",
        "ensemble.fit(X_train_vec, y_train_enc)\n",
        "\n",
        "# Predict\n",
        "y_pred_enc = ensemble.predict(X_test_vec)\n",
        "y_pred = label_encoder.inverse_transform(y_pred_enc)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGUv5CytNtwD",
        "outputId": "4a92a44d-90ca-4658-c380-1e7bad16ac11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.98      0.98       252\n",
            "           I       0.90      0.94      0.92       296\n",
            "           O       1.00      1.00      1.00     11201\n",
            "\n",
            "    accuracy                           1.00     11749\n",
            "   macro avg       0.96      0.97      0.97     11749\n",
            "weighted avg       1.00      1.00      1.00     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9952\n",
            "Precision: 0.9953\n",
            "Recall: 0.9952\n",
            "F1 Score: 0.9953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IOE-tagged data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract features\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare flat X and y\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# Load and prepare\n",
        "sentences, tags = load_excel_data(\"/content/IOE.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize features\n",
        "vec = DictVectorizer(sparse=True)\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# Define base models\n",
        "svm = LinearSVC()\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Ensemble with hard voting\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('svm', svm),\n",
        "        ('tree', tree)\n",
        "    ],\n",
        "    voting='hard'  # You can use 'soft' if classifiers support predict_proba\n",
        ")\n",
        "\n",
        "# Train ensemble\n",
        "ensemble.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ensemble.predict(X_test_vec)\n",
        "\n",
        "print(\"Classification Report (Ensemble SVM + Decision Tree):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIu8U32MOKpn",
        "outputId": "25439c76-a0e9-489c-a6e0-824767da7fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Ensemble SVM + Decision Tree):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E       0.88      0.94      0.91       252\n",
            "           I       0.96      0.97      0.96       293\n",
            "           O       1.00      1.00      1.00     11204\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.95      0.97      0.96     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9946\n",
            "Precision: 0.9948\n",
            "Recall: 0.9946\n",
            "F1 Score: 0.9947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Load IOBES-tagged Excel data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # Sentence boundary\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract features per word\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare data for ML\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# 4. Load data\n",
        "sentences, tags = load_excel_data(\"/content/IOBES.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Initialize vectorizer\n",
        "vec = DictVectorizer(sparse=True)\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# 6. Train classifiers\n",
        "svm_clf = LinearSVC()\n",
        "svm_clf.fit(X_train_vec, y_train)\n",
        "svm_preds = svm_clf.predict(X_test_vec)\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train_vec, y_train)\n",
        "tree_preds = tree_clf.predict(X_test_vec)\n",
        "\n",
        "# 7. Voting ensemble (hard voting)\n",
        "final_preds = []\n",
        "for s_pred, t_pred in zip(svm_preds, tree_preds):\n",
        "    vote = Counter([s_pred, t_pred])\n",
        "    final_preds.append(vote.most_common(1)[0][0])  # majority vote\n",
        "\n",
        "# 8. Evaluation\n",
        "print(\"=== Ensemble Classification Report ===\")\n",
        "print(classification_report(y_test, final_preds))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, final_preds))\n",
        "print(\"Precision:\", precision_score(y_test, final_preds, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, final_preds, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, final_preds, average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BzM2uW4Ol2a",
        "outputId": "30009977-3b32-4ca9-d758-d8016f24c59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Ensemble Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.98      0.99       247\n",
            "           E       0.90      0.92      0.91       248\n",
            "           I       0.95      0.85      0.90        46\n",
            "           O       1.00      1.00      1.00     11204\n",
            "           S       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00     11749\n",
            "   macro avg       0.97      0.95      0.96     11749\n",
            "weighted avg       1.00      1.00      1.00     11749\n",
            "\n",
            "Accuracy: 0.9952336369052686\n",
            "Precision: 0.9952621034658378\n",
            "Recall: 0.9952336369052686\n",
            "F1 Score: 0.9952334009810228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# 1. Load Excel Data with IE Tags\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # Sentence end\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract token-level features\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare data\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# 4. Load and prepare dataset\n",
        "sentences, tags = load_excel_data(\"/content/IE.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Create a DictVectorizer and individual classifiers\n",
        "vec = DictVectorizer(sparse=True)\n",
        "\n",
        "svc = LinearSVC()\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 6. Voting ensemble (hard voting)\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Wrap the classifiers in their own pipelines\n",
        "svc_pipeline = Pipeline([(\"vectorizer\", vec), (\"svc\", svc)])\n",
        "tree_pipeline = Pipeline([(\"vectorizer\", vec), (\"tree\", tree)])\n",
        "\n",
        "# Note: VotingClassifier requires estimators, not pipelines.\n",
        "# We ensemble on transformed vectors (i.e., after DictVectorizer).\n",
        "\n",
        "# Transform features\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# Fit VotingClassifier directly on transformed vectors\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    (\"svc\", svc),\n",
        "    (\"tree\", tree)\n",
        "], voting='hard')\n",
        "\n",
        "voting_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# 7. Predict and evaluate\n",
        "y_pred = voting_clf.predict(X_test_vec)\n",
        "\n",
        "print(\"Classification Report (Ensemble):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results (Ensemble):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lAwIn3hPA4b",
        "outputId": "25dc94d8-4872-4942-8de2-e3d54cdc2241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Ensemble):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E       0.88      0.94      0.91       252\n",
            "          EO       0.94      0.95      0.95       264\n",
            "           I       0.96      0.97      0.96       293\n",
            "          IO       1.00      1.00      1.00     10940\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.95      0.96      0.95     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results (Ensemble):\n",
            "Accuracy: 0.9923\n",
            "Precision: 0.9924\n",
            "Recall: 0.9923\n",
            "F1 Score: 0.9923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from collections import Counter\n",
        "\n",
        "# Your existing functions for loading data and feature extraction here (load_excel_data, word2features, prepare_data)...\n",
        "\n",
        "# Load BI-tagged data\n",
        "sentences, tags = load_excel_data(\"/content/BI.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM pipeline\n",
        "vec = DictVectorizer(sparse=True)\n",
        "svm_clf = LinearSVC()\n",
        "svm_pipeline = make_pipeline(vec, svm_clf)\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Train Decision Tree pipeline\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_pipeline = make_pipeline(vec, dt_clf)\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions from both models\n",
        "svm_preds = svm_pipeline.predict(X_test)\n",
        "dt_preds = dt_pipeline.predict(X_test)\n",
        "\n",
        "# Ensemble by hard voting (majority vote)\n",
        "ensemble_preds = []\n",
        "for p1, p2 in zip(svm_preds, dt_preds):\n",
        "    votes = [p1, p2]\n",
        "    most_common_label = Counter(votes).most_common(1)[0][0]\n",
        "    ensemble_preds.append(most_common_label)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report for Ensemble:\")\n",
        "print(classification_report(y_test, ensemble_preds))\n",
        "\n",
        "print(\"Ensemble Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, ensemble_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IL8UlsDPaG2",
        "outputId": "9987b60c-c246-4643-8274-93ae32d127c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Ensemble:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.98      0.98       251\n",
            "          BO       0.76      0.80      0.78       197\n",
            "           I       0.92      0.93      0.93       294\n",
            "          IO       0.99      0.99      0.99     11007\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.91      0.92      0.92     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Ensemble Evaluation Results:\n",
            "Accuracy: 0.9878\n",
            "Precision: 0.9880\n",
            "Recall: 0.9878\n",
            "F1 Score: 0.9879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1. Load Excel with BIES Tags\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # Sentence end\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract Features for Each Token\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True  # Beginning of sentence\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True  # End of sentence\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Format Features and Labels\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# 4. Majority vote for string labels\n",
        "def majority_vote(preds_array):\n",
        "    \"\"\"\n",
        "    preds_array: numpy array of shape (n_models, n_samples) with string labels\n",
        "    returns: numpy array of shape (n_samples,) with majority vote per sample\n",
        "    \"\"\"\n",
        "    n_samples = preds_array.shape[1]\n",
        "    ensemble_preds = []\n",
        "    for i in range(n_samples):\n",
        "        labels, counts = np.unique(preds_array[:, i], return_counts=True)\n",
        "        majority_label = labels[np.argmax(counts)]\n",
        "        ensemble_preds.append(majority_label)\n",
        "    return np.array(ensemble_preds)\n",
        "\n",
        "# 5. Load data\n",
        "sentences, tags = load_excel_data(\"/content/BIES.xlsx\")  # update to your file path\n",
        "\n",
        "# 6. Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "\n",
        "# 7. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# 8. Create pipelines\n",
        "vec = DictVectorizer(sparse=True)\n",
        "\n",
        "svc_clf = LinearSVC()\n",
        "svc_pipeline = make_pipeline(vec, svc_clf)\n",
        "svc_pipeline.fit(X_train, y_train)\n",
        "svc_preds = svc_pipeline.predict(X_test)\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_pipeline = make_pipeline(vec, dt_clf)\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "dt_preds = dt_pipeline.predict(X_test)\n",
        "\n",
        "# 9. Ensemble predictions via majority vote\n",
        "preds = np.vstack([svc_preds, dt_preds])\n",
        "ensemble_preds = majority_vote(preds)\n",
        "\n",
        "# 10. Evaluation\n",
        "print(\"Classification Report (LinearSVC):\")\n",
        "print(classification_report(y_test, svc_preds))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, svc_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, svc_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, svc_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, svc_preds, average='weighted'):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Decision Tree):\")\n",
        "print(classification_report(y_test, dt_preds))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, dt_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, dt_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, dt_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, dt_preds, average='weighted'):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Ensemble):\")\n",
        "print(classification_report(y_test, ensemble_preds))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, ensemble_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCF_njJZPpNl",
        "outputId": "f3098662-0a1c-46e3-ebbc-01db3bf686af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (LinearSVC):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.98      0.98       247\n",
            "          BO       0.77      0.79      0.78       190\n",
            "           E       0.89      0.92      0.91       248\n",
            "          EO       0.96      0.93      0.94       257\n",
            "           I       0.95      0.85      0.90        46\n",
            "          IO       0.99      0.99      0.99     10750\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       0.67      0.57      0.62         7\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.90      0.88      0.89     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Accuracy: 0.9851\n",
            "Precision: 0.9852\n",
            "Recall: 0.9851\n",
            "F1 Score: 0.9851\n",
            "\n",
            "Classification Report (Decision Tree):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.98      0.98       247\n",
            "          BO       0.66      0.66      0.66       190\n",
            "           E       0.88      0.92      0.90       248\n",
            "          EO       0.95      0.94      0.94       257\n",
            "           I       0.85      0.89      0.87        46\n",
            "          IO       0.99      0.99      0.99     10750\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       0.67      0.29      0.40         7\n",
            "\n",
            "    accuracy                           0.98     11749\n",
            "   macro avg       0.87      0.83      0.84     11749\n",
            "weighted avg       0.98      0.98      0.98     11749\n",
            "\n",
            "Accuracy: 0.9811\n",
            "Precision: 0.9811\n",
            "Recall: 0.9811\n",
            "F1 Score: 0.9810\n",
            "\n",
            "Classification Report (Ensemble):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.98      0.98       247\n",
            "          BO       0.66      0.86      0.75       190\n",
            "           E       0.88      0.93      0.91       248\n",
            "          EO       0.93      0.95      0.94       257\n",
            "           I       0.87      0.89      0.88        46\n",
            "          IO       0.99      0.99      0.99     10750\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       0.67      0.29      0.40         7\n",
            "\n",
            "    accuracy                           0.98     11749\n",
            "   macro avg       0.87      0.86      0.86     11749\n",
            "weighted avg       0.98      0.98      0.98     11749\n",
            "\n",
            "Accuracy: 0.9826\n",
            "Precision: 0.9843\n",
            "Recall: 0.9826\n",
            "F1 Score: 0.9832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load Excel Data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # Sentence end\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Feature extraction per token\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True  # Beginning of sentence\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True  # End of sentence\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare dataset\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# 4. Load and split data\n",
        "sentences, tags = load_excel_data(\"/content/BIES.xlsx\")  # <-- Update to your file path\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Train LinearSVC model\n",
        "vec_svm = DictVectorizer(sparse=True)\n",
        "clf_svm = LinearSVC()\n",
        "pipeline_svm = make_pipeline(vec_svm, clf_svm)\n",
        "pipeline_svm.fit(X_train, y_train)\n",
        "y_pred_svm = pipeline_svm.predict(X_test)\n",
        "\n",
        "# 6. Train Decision Tree model\n",
        "vec_dt = DictVectorizer(sparse=True)\n",
        "clf_dt = DecisionTreeClassifier(random_state=42)\n",
        "pipeline_dt = make_pipeline(vec_dt, clf_dt)\n",
        "pipeline_dt.fit(X_train, y_train)\n",
        "y_pred_dt = pipeline_dt.predict(X_test)\n",
        "\n",
        "# 7. Ensemble Majority Voting\n",
        "preds = np.array([y_pred_svm, y_pred_dt])  # shape: (2, n_samples)\n",
        "preds_T = preds.T  # shape: (n_samples, 2)\n",
        "\n",
        "ensemble_preds = []\n",
        "for sample_preds in preds_T:\n",
        "    most_common = Counter(sample_preds).most_common(1)[0][0]\n",
        "    ensemble_preds.append(most_common)\n",
        "\n",
        "ensemble_preds = np.array(ensemble_preds)\n",
        "\n",
        "# 8. Evaluation Reports\n",
        "\n",
        "print(\"Classification Report (LinearSVC):\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "print(\"Classification Report (Decision Tree):\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "print(\"Classification Report (Ensemble):\")\n",
        "print(classification_report(y_test, ensemble_preds))\n",
        "\n",
        "print(\"Ensemble Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, ensemble_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1j2vYKbQVvj",
        "outputId": "3daca828-eba7-44ef-f183-b2d4633e7c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (LinearSVC):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.98      0.98       247\n",
            "          BO       0.77      0.79      0.78       190\n",
            "           E       0.89      0.92      0.91       248\n",
            "          EO       0.96      0.93      0.94       257\n",
            "           I       0.95      0.85      0.90        46\n",
            "          IO       0.99      0.99      0.99     10750\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       0.67      0.57      0.62         7\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.90      0.88      0.89     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Classification Report (Decision Tree):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.98      0.98       247\n",
            "          BO       0.66      0.66      0.66       190\n",
            "           E       0.88      0.92      0.90       248\n",
            "          EO       0.95      0.94      0.94       257\n",
            "           I       0.85      0.89      0.87        46\n",
            "          IO       0.99      0.99      0.99     10750\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       0.67      0.29      0.40         7\n",
            "\n",
            "    accuracy                           0.98     11749\n",
            "   macro avg       0.87      0.83      0.84     11749\n",
            "weighted avg       0.98      0.98      0.98     11749\n",
            "\n",
            "Classification Report (Ensemble):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.98      0.98       247\n",
            "          BO       0.77      0.79      0.78       190\n",
            "           E       0.89      0.92      0.91       248\n",
            "          EO       0.96      0.93      0.94       257\n",
            "           I       0.95      0.85      0.90        46\n",
            "          IO       0.99      0.99      0.99     10750\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       0.67      0.57      0.62         7\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.90      0.88      0.89     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Ensemble Evaluation Results:\n",
            "Accuracy: 0.9851\n",
            "Precision: 0.9852\n",
            "Recall: 0.9851\n",
            "F1 Score: 0.9851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SECOND"
      ],
      "metadata": {
        "id": "V2OGaLNgwO0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble of SVM and Decision Tree using VotingClassifier\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# 1. Load Excel Data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # End of sentence\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Feature extraction per token\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare dataset\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# 4. Load and split data\n",
        "sentences, tags = load_excel_data(\"/content/IO2.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Vectorize features\n",
        "vec = DictVectorizer(sparse=True)\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# 6. Encode labels\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_test_enc = le.transform(y_test)\n",
        "\n",
        "# 7. Define base classifiers\n",
        "svm = LinearSVC(random_state=42)\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 8. Ensemble via hard voting\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('svm', svm),\n",
        "    ('dt', dt)\n",
        "], voting='hard')\n",
        "\n",
        "# 9. Fit and predict\n",
        "ensemble.fit(X_train_vec, y_train_enc)\n",
        "y_pred_enc = ensemble.predict(X_test_vec)\n",
        "y_pred = le.inverse_transform(y_pred_enc)\n",
        "\n",
        "# 10. Evaluate\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92T4A1SpwQAM",
        "outputId": "46fd7c42-b25a-45dc-d68d-fb96c68d750f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.93      0.96      0.94       591\n",
            "           O       1.00      1.00      1.00     11158\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.96      0.98      0.97     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9941\n",
            "Precision: 0.9943\n",
            "Recall: 0.9941\n",
            "F1 Score: 0.9942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 1. Load Excel Data with IOB Tags\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # End of sentence\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract features per token\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare token-level data\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# Load data\n",
        "sentences, tags = load_excel_data(\"/content/IOB2.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode labels (VotingClassifier doesn't support string labels directly)\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_enc = label_encoder.fit_transform(y_train)\n",
        "y_test_enc = label_encoder.transform(y_test)\n",
        "\n",
        "# Vectorizer\n",
        "vec = DictVectorizer(sparse=True)\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# Base classifiers\n",
        "svm = LinearSVC()\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Voting Classifier (Hard voting)\n",
        "ensemble = VotingClassifier(estimators=[\n",
        "    ('svm', svm),\n",
        "    ('tree', tree)\n",
        "], voting='hard')\n",
        "\n",
        "# Train\n",
        "ensemble.fit(X_train_vec, y_train_enc)\n",
        "\n",
        "# Predict\n",
        "y_pred_enc = ensemble.predict(X_test_vec)\n",
        "y_pred = label_encoder.inverse_transform(y_pred_enc)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQdu61inwkLW",
        "outputId": "9d074b58-b8bf-4fc8-8b45-c3557880fc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.99      0.99       266\n",
            "           I       0.89      0.94      0.91       325\n",
            "           O       1.00      1.00      1.00     11158\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.96      0.97      0.97     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9945\n",
            "Precision: 0.9946\n",
            "Recall: 0.9945\n",
            "F1 Score: 0.9945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# 1. Load IOE-tagged data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract features\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare flat X and y\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# Load and prepare\n",
        "sentences, tags = load_excel_data(\"/content/IOE2.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize features\n",
        "vec = DictVectorizer(sparse=True)\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# Define base models\n",
        "svm = LinearSVC()\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Ensemble with hard voting\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('svm', svm),\n",
        "        ('tree', tree)\n",
        "    ],\n",
        "    voting='hard'  # You can use 'soft' if classifiers support predict_proba\n",
        ")\n",
        "\n",
        "# Train ensemble\n",
        "ensemble.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = ensemble.predict(X_test_vec)\n",
        "\n",
        "print(\"Classification Report (Ensemble SVM + Decision Tree):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHS8hMS3wopW",
        "outputId": "2fe6857f-6d52-4808-d515-b5292729c91a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Ensemble SVM + Decision Tree):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E       0.89      0.92      0.91       275\n",
            "           I       0.96      0.98      0.97       316\n",
            "           O       1.00      1.00      1.00     11158\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.95      0.97      0.96     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9943\n",
            "Precision: 0.9944\n",
            "Recall: 0.9943\n",
            "F1 Score: 0.9943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Load IOBES-tagged Excel data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # Sentence boundary\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract features per word\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare data for ML\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# 4. Load data\n",
        "sentences, tags = load_excel_data(\"/content/IOBES2.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Initialize vectorizer\n",
        "vec = DictVectorizer(sparse=True)\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# 6. Train classifiers\n",
        "svm_clf = LinearSVC()\n",
        "svm_clf.fit(X_train_vec, y_train)\n",
        "svm_preds = svm_clf.predict(X_test_vec)\n",
        "\n",
        "tree_clf = DecisionTreeClassifier(random_state=42)\n",
        "tree_clf.fit(X_train_vec, y_train)\n",
        "tree_preds = tree_clf.predict(X_test_vec)\n",
        "\n",
        "# 7. Voting ensemble (hard voting)\n",
        "final_preds = []\n",
        "for s_pred, t_pred in zip(svm_preds, tree_preds):\n",
        "    vote = Counter([s_pred, t_pred])\n",
        "    final_preds.append(vote.most_common(1)[0][0])  # majority vote\n",
        "\n",
        "# 8. Evaluation\n",
        "print(\"=== Ensemble Classification Report ===\")\n",
        "print(classification_report(y_test, final_preds))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, final_preds))\n",
        "print(\"Precision:\", precision_score(y_test, final_preds, average='weighted'))\n",
        "print(\"Recall:\", recall_score(y_test, final_preds, average='weighted'))\n",
        "print(\"F1 Score:\", f1_score(y_test, final_preds, average='weighted'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sctq-W5SwsvL",
        "outputId": "5fa76c0c-e07a-46f0-8cbd-9a5a875fe2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Ensemble Classification Report ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.99      0.99       262\n",
            "           E       0.93      0.92      0.92       271\n",
            "           I       0.86      0.93      0.89        54\n",
            "           O       1.00      1.00      1.00     11158\n",
            "           S       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00     11749\n",
            "   macro avg       0.96      0.97      0.96     11749\n",
            "weighted avg       1.00      1.00      1.00     11749\n",
            "\n",
            "Accuracy: 0.995148523278577\n",
            "Precision: 0.995171255421157\n",
            "Recall: 0.995148523278577\n",
            "F1 Score: 0.9951534727964327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# 1. Load Excel Data with IE Tags\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # Sentence end\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract token-level features\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare data\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# 4. Load and prepare dataset\n",
        "sentences, tags = load_excel_data(\"/content/IE2.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Create a DictVectorizer and individual classifiers\n",
        "vec = DictVectorizer(sparse=True)\n",
        "\n",
        "svc = LinearSVC()\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 6. Voting ensemble (hard voting)\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# Wrap the classifiers in their own pipelines\n",
        "svc_pipeline = Pipeline([(\"vectorizer\", vec), (\"svc\", svc)])\n",
        "tree_pipeline = Pipeline([(\"vectorizer\", vec), (\"tree\", tree)])\n",
        "\n",
        "# Note: VotingClassifier requires estimators, not pipelines.\n",
        "# We ensemble on transformed vectors (i.e., after DictVectorizer).\n",
        "\n",
        "# Transform features\n",
        "X_train_vec = vec.fit_transform(X_train)\n",
        "X_test_vec = vec.transform(X_test)\n",
        "\n",
        "# Fit VotingClassifier directly on transformed vectors\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    (\"svc\", svc),\n",
        "    (\"tree\", tree)\n",
        "], voting='hard')\n",
        "\n",
        "voting_clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# 7. Predict and evaluate\n",
        "y_pred = voting_clf.predict(X_test_vec)\n",
        "\n",
        "print(\"Classification Report (Ensemble):\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results (Ensemble):\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZa2VBrVww8Y",
        "outputId": "21d8f97a-42f7-43c3-df10-64091a5f6c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (Ensemble):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E       0.89      0.92      0.91       275\n",
            "          EO       0.94      0.95      0.94       274\n",
            "           I       0.96      0.99      0.97       316\n",
            "          IO       1.00      0.99      1.00     10884\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.95      0.96      0.95     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results (Ensemble):\n",
            "Accuracy: 0.9917\n",
            "Precision: 0.9918\n",
            "Recall: 0.9917\n",
            "F1 Score: 0.9917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from collections import Counter\n",
        "\n",
        "# Your existing functions for loading data and feature extraction here (load_excel_data, word2features, prepare_data)...\n",
        "\n",
        "# Load BI-tagged data\n",
        "sentences, tags = load_excel_data(\"/content/BI2.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM pipeline\n",
        "vec = DictVectorizer(sparse=True)\n",
        "svm_clf = LinearSVC()\n",
        "svm_pipeline = make_pipeline(vec, svm_clf)\n",
        "svm_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Train Decision Tree pipeline\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_pipeline = make_pipeline(vec, dt_clf)\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions from both models\n",
        "svm_preds = svm_pipeline.predict(X_test)\n",
        "dt_preds = dt_pipeline.predict(X_test)\n",
        "\n",
        "# Ensemble by hard voting (majority vote)\n",
        "ensemble_preds = []\n",
        "for p1, p2 in zip(svm_preds, dt_preds):\n",
        "    votes = [p1, p2]\n",
        "    most_common_label = Counter(votes).most_common(1)[0][0]\n",
        "    ensemble_preds.append(most_common_label)\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report for Ensemble:\")\n",
        "print(classification_report(y_test, ensemble_preds))\n",
        "\n",
        "print(\"Ensemble Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, ensemble_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJJu4_C2w3Ls",
        "outputId": "c42baf00-d624-4d27-e45d-989d139a9518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Ensemble:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.99      0.99       266\n",
            "          BO       0.77      0.82      0.79       210\n",
            "           I       0.93      0.92      0.93       325\n",
            "          IO       0.99      0.99      0.99     10948\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.92      0.93      0.93     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Ensemble Evaluation Results:\n",
            "Accuracy: 0.9879\n",
            "Precision: 0.9881\n",
            "Recall: 0.9879\n",
            "F1 Score: 0.9880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1. Load Excel with BIES Tags\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # Sentence end\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Extract Features for Each Token\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True  # Beginning of sentence\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True  # End of sentence\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Format Features and Labels\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# 4. Majority vote for string labels\n",
        "def majority_vote(preds_array):\n",
        "    \"\"\"\n",
        "    preds_array: numpy array of shape (n_models, n_samples) with string labels\n",
        "    returns: numpy array of shape (n_samples,) with majority vote per sample\n",
        "    \"\"\"\n",
        "    n_samples = preds_array.shape[1]\n",
        "    ensemble_preds = []\n",
        "    for i in range(n_samples):\n",
        "        labels, counts = np.unique(preds_array[:, i], return_counts=True)\n",
        "        majority_label = labels[np.argmax(counts)]\n",
        "        ensemble_preds.append(majority_label)\n",
        "    return np.array(ensemble_preds)\n",
        "\n",
        "# 5. Load data\n",
        "sentences, tags = load_excel_data(\"/content/BIES2.xlsx\")  # update to your file path\n",
        "\n",
        "# 6. Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "\n",
        "# 7. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# 8. Create pipelines\n",
        "vec = DictVectorizer(sparse=True)\n",
        "\n",
        "svc_clf = LinearSVC()\n",
        "svc_pipeline = make_pipeline(vec, svc_clf)\n",
        "svc_pipeline.fit(X_train, y_train)\n",
        "svc_preds = svc_pipeline.predict(X_test)\n",
        "\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_pipeline = make_pipeline(vec, dt_clf)\n",
        "dt_pipeline.fit(X_train, y_train)\n",
        "dt_preds = dt_pipeline.predict(X_test)\n",
        "\n",
        "# 9. Ensemble predictions via majority vote\n",
        "preds = np.vstack([svc_preds, dt_preds])\n",
        "ensemble_preds = majority_vote(preds)\n",
        "\n",
        "# 10. Evaluation\n",
        "print(\"Classification Report (LinearSVC):\")\n",
        "print(classification_report(y_test, svc_preds))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, svc_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, svc_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, svc_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, svc_preds, average='weighted'):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Decision Tree):\")\n",
        "print(classification_report(y_test, dt_preds))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, dt_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, dt_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, dt_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, dt_preds, average='weighted'):.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report (Ensemble):\")\n",
        "print(classification_report(y_test, ensemble_preds))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, ensemble_preds):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, ensemble_preds, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTxGO-viw7oI",
        "outputId": "3868f83f-482b-4fdd-d831-f7ad74dfc607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report (LinearSVC):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.99      0.99       262\n",
            "          BO       0.78      0.82      0.80       202\n",
            "           E       0.93      0.92      0.92       271\n",
            "          EO       0.96      0.93      0.94       266\n",
            "           I       0.86      0.93      0.89        54\n",
            "          IO       0.99      0.99      0.99     10682\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       0.67      0.50      0.57         8\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.90      0.88      0.89     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Accuracy: 0.9854\n",
            "Precision: 0.9856\n",
            "Recall: 0.9854\n",
            "F1 Score: 0.9855\n",
            "\n",
            "Classification Report (Decision Tree):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.98      0.99       262\n",
            "          BO       0.72      0.68      0.70       202\n",
            "           E       0.90      0.91      0.90       271\n",
            "          EO       0.94      0.95      0.94       266\n",
            "           I       0.83      0.96      0.89        54\n",
            "          IO       0.99      0.99      0.99     10682\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       1.00      0.25      0.40         8\n",
            "\n",
            "    accuracy                           0.98     11749\n",
            "   macro avg       0.92      0.84      0.85     11749\n",
            "weighted avg       0.98      0.98      0.98     11749\n",
            "\n",
            "Accuracy: 0.9814\n",
            "Precision: 0.9813\n",
            "Recall: 0.9814\n",
            "F1 Score: 0.9812\n",
            "\n",
            "Classification Report (Ensemble):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.99      0.99       262\n",
            "          BO       0.70      0.88      0.78       202\n",
            "           E       0.89      0.92      0.91       271\n",
            "          EO       0.93      0.95      0.94       266\n",
            "           I       0.83      0.96      0.89        54\n",
            "          IO       0.99      0.99      0.99     10682\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       1.00      0.25      0.40         8\n",
            "\n",
            "    accuracy                           0.98     11749\n",
            "   macro avg       0.92      0.87      0.86     11749\n",
            "weighted avg       0.98      0.98      0.98     11749\n",
            "\n",
            "Accuracy: 0.9831\n",
            "Precision: 0.9846\n",
            "Recall: 0.9831\n",
            "F1 Score: 0.9835\n"
          ]
        }
      ]
    }
  ]
}