{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPh0zJIN6z0zVacUYi3A9mJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/its3alih/Thesis/blob/main/hyperparametersSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FIRST"
      ],
      "metadata": {
        "id": "TdUCCkjhpdyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM"
      ],
      "metadata": {
        "id": "NNZ2Xl-2SYyu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9CoJjboRjJs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load Excel Data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # End of sentence\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Feature extraction per token\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True  # Beginning of sentence\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True  # End of sentence\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare dataset\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "sentences, tags = load_excel_data(\"/content/IO.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_DD8kY8SfSw",
        "outputId": "d598b3a4-4b1e-41f6-fa16-bac5b68f7792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.95      0.95      0.95       548\n",
            "           O       1.00      1.00      1.00     11201\n",
            "\n",
            "    accuracy                           1.00     11749\n",
            "   macro avg       0.97      0.97      0.97     11749\n",
            "weighted avg       1.00      1.00      1.00     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9954\n",
            "Precision: 0.9954\n",
            "Recall: 0.9954\n",
            "F1 Score: 0.9954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# (Include your existing functions load_excel_data, word2features, prepare_data here)\n",
        "\n",
        "# Load and prepare data\n",
        "sentences, tags = load_excel_data(\"/content/IOB.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],            # Regularization parameter\n",
        "    'clf__max_iter': [1000, 5000],                # Number of iterations\n",
        "    'clf__class_weight': [None, 'balanced'],      # Handle class imbalance\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit GridSearch on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "# Predict using best estimator\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Evaluate results\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4q7MhKBTZdc",
        "outputId": "6adf25d7-544a-4087-de78-de946cab192d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.98      0.98       252\n",
            "           I       0.93      0.93      0.93       296\n",
            "           O       1.00      1.00      1.00     11201\n",
            "\n",
            "    accuracy                           1.00     11749\n",
            "   macro avg       0.97      0.97      0.97     11749\n",
            "weighted avg       1.00      1.00      1.00     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9957\n",
            "Precision: 0.9956\n",
            "Recall: 0.9957\n",
            "F1 Score: 0.9957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Your existing functions for IOE tags\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # Sentence boundary\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# Load IOE data\n",
        "sentences, tags = load_excel_data(\"/content/IOE.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBdf6f8HTeyN",
        "outputId": "d3946f15-5776-43c6-bf46-0e7bed74e4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E       0.91      0.92      0.92       252\n",
            "           I       0.98      0.97      0.97       293\n",
            "           O       1.00      1.00      1.00     11204\n",
            "\n",
            "    accuracy                           1.00     11749\n",
            "   macro avg       0.96      0.96      0.96     11749\n",
            "weighted avg       1.00      1.00      1.00     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9952\n",
            "Precision: 0.9953\n",
            "Recall: 0.9952\n",
            "F1 Score: 0.9952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IOBES data\n",
        "sentences, tags = load_excel_data(\"/content/IOBES.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid (same as before)\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh4AUoz3WLoE",
        "outputId": "29867e38-45aa-4f96-e88b-d5948cd8ec4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.98      0.99       247\n",
            "           E       0.90      0.92      0.91       248\n",
            "           I       0.95      0.85      0.90        46\n",
            "           O       1.00      1.00      1.00     11204\n",
            "           S       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00     11749\n",
            "   macro avg       0.97      0.95      0.96     11749\n",
            "weighted avg       1.00      1.00      1.00     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9952\n",
            "Precision: 0.9953\n",
            "Recall: 0.9952\n",
            "F1 Score: 0.9952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IE data\n",
        "sentences, tags = load_excel_data(\"/content/IE.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynZmI6XYXqrb",
        "outputId": "0ce7e598-2f18-4cf1-bf0f-a4a3dcef7551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E       0.90      0.92      0.91       252\n",
            "          EO       0.96      0.93      0.95       264\n",
            "           I       0.98      0.97      0.97       293\n",
            "          IO       1.00      1.00      1.00     10940\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.96      0.95      0.96     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9928\n",
            "Precision: 0.9928\n",
            "Recall: 0.9928\n",
            "F1 Score: 0.9928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BI data\n",
        "sentences, tags = load_excel_data(\"/content/BI.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isXmALW8Yt6i",
        "outputId": "47df64f6-5a1d-4e2f-f0b7-26b3bb807604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 10, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.98      0.98       251\n",
            "          BO       0.76      0.81      0.79       197\n",
            "           I       0.90      0.93      0.91       294\n",
            "          IO       0.99      0.99      0.99     11007\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.91      0.93      0.92     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9873\n",
            "Precision: 0.9877\n",
            "Recall: 0.9873\n",
            "F1 Score: 0.9875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BIES data\n",
        "sentences, tags = load_excel_data(\"/content/BIES.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLqzn7svZAtn",
        "outputId": "3747148f-9394-4022-f45c-2f6106d9dc74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.98      0.98      0.98       247\n",
            "          BO       0.77      0.79      0.78       190\n",
            "           E       0.89      0.92      0.91       248\n",
            "          EO       0.96      0.93      0.94       257\n",
            "           I       0.95      0.85      0.90        46\n",
            "          IO       0.99      0.99      0.99     10750\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       0.67      0.57      0.62         7\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.90      0.88      0.89     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9851\n",
            "Precision: 0.9852\n",
            "Recall: 0.9851\n",
            "F1 Score: 0.9851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SECOND"
      ],
      "metadata": {
        "id": "zwFaUc6hpgjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load Excel Data\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # End of sentence\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# 2. Feature extraction per token\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True  # Beginning of sentence\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True  # End of sentence\n",
        "\n",
        "    return features\n",
        "\n",
        "# 3. Prepare dataset\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n"
      ],
      "metadata": {
        "id": "yg4C9hdEph3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "sentences, tags = load_excel_data(\"/content/IO2.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR0SXjsEppQp",
        "outputId": "cfc2b060-9bcf-4847-8df7-a18c950c9082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           I       0.95      0.95      0.95       591\n",
            "           O       1.00      1.00      1.00     11158\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.97      0.97      0.97     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9950\n",
            "Precision: 0.9950\n",
            "Recall: 0.9950\n",
            "F1 Score: 0.9950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# (Include your existing functions load_excel_data, word2features, prepare_data here)\n",
        "\n",
        "# Load and prepare data\n",
        "sentences, tags = load_excel_data(\"/content/IOB2.xlsx\")\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],            # Regularization parameter\n",
        "    'clf__max_iter': [1000, 5000],                # Number of iterations\n",
        "    'clf__class_weight': [None, 'balanced'],      # Handle class imbalance\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit GridSearch on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print best parameters\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "# Predict using best estimator\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "# Evaluate results\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKz5u9wxpv93",
        "outputId": "79af773b-ea77-402f-f355-0dfc0ee9906d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 10, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.99      0.99       266\n",
            "           I       0.91      0.92      0.91       325\n",
            "           O       1.00      1.00      1.00     11158\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.96      0.97      0.97     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9946\n",
            "Precision: 0.9947\n",
            "Recall: 0.9946\n",
            "F1 Score: 0.9947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Your existing functions for IOE tags\n",
        "def load_excel_data(file_path):\n",
        "    df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)\n",
        "    df = df[['Word i', 'Word i entity tag']].dropna()\n",
        "\n",
        "    sentences, labels = [], []\n",
        "    sentence, label = [], []\n",
        "    for word, tag in zip(df['Word i'], df['Word i entity tag']):\n",
        "        if str(word).strip() in ['.', '؟']:  # Sentence boundary\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                labels.append(label)\n",
        "                sentence, label = [], []\n",
        "        else:\n",
        "            sentence.append(str(word).strip())\n",
        "            label.append(str(tag).strip())\n",
        "\n",
        "    if sentence:\n",
        "        sentences.append(sentence)\n",
        "        labels.append(label)\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "def word2features(sent, i):\n",
        "    word = sent[i]\n",
        "    features = {\n",
        "        'word': word,\n",
        "        'is_upper': word.isupper(),\n",
        "        'is_title': word.istitle(),\n",
        "        'is_digit': word.isdigit(),\n",
        "    }\n",
        "    if i > 0:\n",
        "        word1 = sent[i - 1]\n",
        "        features.update({\n",
        "            '-1:word': word1,\n",
        "            '-1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['BOS'] = True\n",
        "\n",
        "    if i < len(sent) - 1:\n",
        "        word1 = sent[i + 1]\n",
        "        features.update({\n",
        "            '+1:word': word1,\n",
        "            '+1:is_title': word1.istitle(),\n",
        "        })\n",
        "    else:\n",
        "        features['EOS'] = True\n",
        "\n",
        "    return features\n",
        "\n",
        "def prepare_data(sentences, labels):\n",
        "    X, y = [], []\n",
        "    for sent, label_seq in zip(sentences, labels):\n",
        "        for i in range(len(sent)):\n",
        "            feats = word2features(sent, i)\n",
        "            X.append(feats)\n",
        "            y.append(label_seq[i])\n",
        "    return X, y\n",
        "\n",
        "# Load IOE data\n",
        "sentences, tags = load_excel_data(\"/content/IOE2.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg-dRgexp10T",
        "outputId": "1635254b-2bc8-4964-fbce-5839ba6699c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 10, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E       0.93      0.92      0.92       275\n",
            "           I       0.96      0.97      0.97       316\n",
            "           O       1.00      1.00      1.00     11158\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.96      0.96      0.96     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9946\n",
            "Precision: 0.9946\n",
            "Recall: 0.9946\n",
            "F1 Score: 0.9946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IOBES data\n",
        "sentences, tags = load_excel_data(\"/content/IOBES2.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid (same as before)\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRjEBo8Jp4JP",
        "outputId": "8d6cfc4d-a151-4227-8609-644f2ef6d8f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.99      0.99       262\n",
            "           E       0.93      0.92      0.92       271\n",
            "           I       0.86      0.93      0.89        54\n",
            "           O       1.00      1.00      1.00     11158\n",
            "           S       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00     11749\n",
            "   macro avg       0.96      0.97      0.96     11749\n",
            "weighted avg       1.00      1.00      1.00     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9951\n",
            "Precision: 0.9952\n",
            "Recall: 0.9951\n",
            "F1 Score: 0.9952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IE data\n",
        "sentences, tags = load_excel_data(\"/content/IE2.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZXXczTTp8o2",
        "outputId": "c60ea21e-de68-4249-b246-fad9a0302617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           E       0.93      0.91      0.92       275\n",
            "          EO       0.97      0.92      0.94       274\n",
            "           I       0.97      0.98      0.98       316\n",
            "          IO       1.00      1.00      1.00     10884\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.97      0.95      0.96     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9928\n",
            "Precision: 0.9927\n",
            "Recall: 0.9928\n",
            "F1 Score: 0.9927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BI data\n",
        "sentences, tags = load_excel_data(\"/content/BI2.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkZ9mBirp-4q",
        "outputId": "8efe2a44-c2ee-4a3e-94bc-34096bb264df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 10, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.98      0.99       266\n",
            "          BO       0.79      0.81      0.80       210\n",
            "           I       0.92      0.92      0.92       325\n",
            "          IO       0.99      0.99      0.99     10948\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.92      0.93      0.93     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9878\n",
            "Precision: 0.9879\n",
            "Recall: 0.9878\n",
            "F1 Score: 0.9879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BIES data\n",
        "sentences, tags = load_excel_data(\"/content/BIES2.xlsx\")\n",
        "\n",
        "# Prepare data\n",
        "X_all, y_all = prepare_data(sentences, tags)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('vectorizer', DictVectorizer(sparse=True)),\n",
        "    ('clf', LinearSVC())\n",
        "])\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    'clf__C': [0.01, 0.1, 1, 10, 100],\n",
        "    'clf__max_iter': [1000, 5000],\n",
        "    'clf__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    pipeline, param_grid, cv=5, scoring='f1_weighted', n_jobs=-1, verbose=2\n",
        ")\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, average='weighted'):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, average='weighted'):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwDl7-WTqDb0",
        "outputId": "f84d860e-68e2-4747-d675-a4df39e75838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best parameters found: {'clf__C': 1, 'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           B       0.99      0.99      0.99       262\n",
            "          BO       0.78      0.82      0.80       202\n",
            "           E       0.93      0.92      0.92       271\n",
            "          EO       0.96      0.93      0.94       266\n",
            "           I       0.86      0.93      0.89        54\n",
            "          IO       0.99      0.99      0.99     10682\n",
            "           S       1.00      1.00      1.00         4\n",
            "          SO       0.67      0.50      0.57         8\n",
            "\n",
            "    accuracy                           0.99     11749\n",
            "   macro avg       0.90      0.88      0.89     11749\n",
            "weighted avg       0.99      0.99      0.99     11749\n",
            "\n",
            "Evaluation Results:\n",
            "Accuracy: 0.9854\n",
            "Precision: 0.9856\n",
            "Recall: 0.9854\n",
            "F1 Score: 0.9855\n"
          ]
        }
      ]
    }
  ]
}